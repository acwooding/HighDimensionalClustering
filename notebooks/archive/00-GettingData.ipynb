{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e429df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pulling data locally\n",
    "\n",
    "This notebook will copy the datasets into the data_folder path. All the other notebooks will read the data from this folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed84d6a-eeca-4f9f-afc1-cc11b131d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d3bdf4-9b88-4008-93e6-5e6f656e5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import paths\n",
    "data_folder = paths['data_path']\n",
    "print(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a03c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob\n",
    "import re\n",
    "import rarfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab73773",
   "metadata": {},
   "source": [
    "# MNIST, USPS and Pendigits are easy\n",
    "\n",
    "We can use the sklearn API to fetch data for the Pendigits, MNIST and USPS datasets.\n",
    "\n",
    "Of these datasets pendigits is the smallest, with only 1797 samples, and is only 64 dimensional. This makes a good first dataset to test things out on -- the dataset is small enough that practically anything should be able to run on this efficiently.\n",
    "\n",
    "USPS provides a slightly more challenging dataset, with almost 10,000 samples and 256 dimensions, but is still samall enough to be tractable for even naive clustering implementations.\n",
    "\n",
    "MNIST provides a good basic scaling test with 70,000 samples in 784 dimensions. In practice this is not a very large dataset compared to many that people want to cluster, although the dimensionality may provide some challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "mnist = fetch_openml(\"MNIST_784\")\n",
    "usps = fetch_openml(\"USPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc2443",
   "metadata": {},
   "source": [
    "# Buildings and COIL are harder\n",
    "\n",
    "The buildings and COIL-20 datasets provide some slightly more challenging image based problems, with more complex images to be dealt with. Both are still small in number of samples, so should be easily tractable. COIL *should* be relatively easy to cluster since the different classes should provide fairly tight and distinct clusters (being 72 images of the same object from different angles for each class). The buildings dataset, which has colour images from many angles and different lighting conditions, should be a much more challenging problem to cluster if using simple euclidean distance on the flattened vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752902f-6990-48f1-a609-4e406601c33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(data_folder):\n",
    "    bashCommand = f\"mkdir {data_folder}\"\n",
    "    os.system(bashCommand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2c7ca",
   "metadata": {},
   "source": [
    "## Coil-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if not os.path.isfile(data_folder / 'coil20.zip'):\n",
    "    results = requests.get('http://www.cs.columbia.edu/CAVE/databases/SLAM_coil-20_coil-100/coil-20/coil-20-proc.zip')\n",
    "    with open(data_folder / 'coil20.zip', \"wb\") as code:\n",
    "        code.write(results.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0090d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_zip = zipfile.ZipFile(data_folder / 'coil20.zip')\n",
    "mylist = images_zip.namelist()\n",
    "r = re.compile(\".*\\.png$\")\n",
    "filelist = list(filter(r.match, mylist))\n",
    "images_zip.extractall(str(data_folder) + '/.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492310b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "coil_feature_vectors = []\n",
    "for filename in filelist:\n",
    "    im = imageio.imread(data_folder / filename)\n",
    "    coil_feature_vectors.append(im.flatten())\n",
    "coil_20_data = np.asarray(coil_feature_vectors)\n",
    "coil_20_target = pd.Series(filelist).str.extract(\"obj([0-9]+)\", expand=False).values.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340778c",
   "metadata": {},
   "source": [
    "## Buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02ef67",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(data_folder / 'buildings.rar'):\n",
    "    results = requests.get('http://eprints.lincoln.ac.uk/id/eprint/16079/1/dataset.rar')\n",
    "    with open(data_folder / 'buildings.rar', \"wb\") as code:\n",
    "        code.write(results.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e58ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(data_folder / 'sheffield_buildings/Dataset/Dataset/1/S1-01.jpeg'):\n",
    "    rf = rarfile.RarFile(f'{data_folder}/buildings.rar')\n",
    "    rf.extractall(f'{data_folder}/sheffield_buildings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "buildings_data = []\n",
    "buildings_target = []\n",
    "for i in range(1, 41):\n",
    "    directory = data_folder / f\"sheffield_buildings/Dataset/Dataset/{i}\"\n",
    "    images = np.vstack([np.asarray(Image.open(filename).resize((96, 96))).flatten() for filename in glob(f\"{directory}/*\")])\n",
    "    labels = np.full(len(glob(f\"{directory}/*\")), i, dtype=np.int32)\n",
    "    buildings_data.append(images)\n",
    "    buildings_target.append(labels)\n",
    "buildings_data = np.vstack(buildings_data)\n",
    "buildings_target = np.hstack(buildings_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303fbd9-4af1-4a61-a5a9-4c44d27d7510",
   "metadata": {},
   "source": [
    "## Clusterable Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3b11ea-d152-4097-bc94-4735f0894bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(data_folder / 'clusterable_data.npy'):\n",
    "    git_repo_url = 'https://github.com/scikit-learn-contrib/hdbscan/blob/master/notebooks/clusterable_data.npy?raw=true'\n",
    "    urllib.request.urlretrieve(git_repo_url, filename=f\"{data_folder}/clusterable_data.npy\")\n",
    "data= np.load(f'{data_folder}/clusterable_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f629b48-8878-4991-b81c-b22e4475aef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HighDimensionalClustering",
   "language": "python",
   "name": "highdimensionalclustering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
