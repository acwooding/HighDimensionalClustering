{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e429df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clustering evaluation on high dimensional data using graph clustering\n",
    "\n",
    "The goal of this notebook is to compare different graph clustering algorithms on the UMAP graphs (fuzzy intersection, fuzzy union, with and without edge weights).\n",
    "\n",
    "We have run standard graph clustering algorithms (Louvain, Leiden, ECG and label propagation) on different graphs obtainable from the high dimensional data. The 3 graphs we focus on are:\n",
    "* The fuzzy simplicial graph, from fuzzy unions (this is like the undirected graph obtained from the directed k-NN graph with weights being the maximal UMAP weight of directed edges)\n",
    "* The fuzzy simplicial graph, from fuzzy unions (same as above, but with the minimum weight. This implies that only bi-directional edges of the k-NN graph are present.)\n",
    "* The k-NN of the low-dimensional representation (after a UMAP dimension reduction)\n",
    "\n",
    "Results show that Leiden and Louvain are the two clustering techniques that perform the best for this task. But it is generally not as good as applying HDBSCAN on the low dimension vectors obtained from UMAP.\n",
    "\n",
    "Furthermore, the graph to use (fuzzy union or intersection, weighted or not, ...) to get the best clustering results differs from one dataset to the other. \n",
    "* Pendits: Weighted fuzzy union graph yields better results (even better than the lower dimensional k-NN graph)\n",
    "* Mnist and USPS: unweighted fuzzy union graph yields better results.\n",
    "* Coil: The low dim k-NN graph yields much better results than any other graph.\n",
    "* Buildings: the unweighted fuzzy intersection yields better results, even better than HDBSCAN+UMAP.\n",
    "\n",
    "We have also tested if the difference in performance between graph clustering algorithms and HDBSCAN could come from the fact that HDBSCAN has a \"noise\" category, which reduces the data set size used for evaluating the technique. We've tested this in two ways. First, we have restricted all evaluation to the same set, second, we have completely removed the noise points (w.r.t. HDBSCAN) from the original dataset (this is cheating: identifying the noise points is a challenge and, in our algorithm list, only HDBSCAN does it). Removing noise, either from evaluation or from the dataset, does improve the performance of all algorithms, but it does not change the how the algorithms compare to one another.\n",
    "\n",
    "## Varying number of NNs when building graph\n",
    "\n",
    "We also tried to vary the number of NNs when constructing the graph and running a clustering algorithm on the resulting graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4875757-c802-423c-8bf8-8efd45ff78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  master\u001b[m\n",
      "* \u001b[32mpruning_graph\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d3bdf4-9b88-4008-93e6-5e6f656e5b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pendigits', 'coil', 'mnist', 'usps', 'buildings', 'clusterable']\n"
     ]
    }
   ],
   "source": [
    "execfile('functions/data_specifics.py')\n",
    "execfile('functions/graph_functions.py')\n",
    "print(data_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a03c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n",
    "from sklearn import cluster\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import hdbscan\n",
    "import umap\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "import pynndescent\n",
    "\n",
    "from partition_igraph import community_ecg as ecg\n",
    "from community import community_louvain, modularity\n",
    "import leidenalg as la\n",
    "import igraph as ig\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab73773",
   "metadata": {},
   "source": [
    "# MNIST, USPS and Pendigits are easy\n",
    "\n",
    "We can use the sklearn API to fetch data for the Pendigits, MNIST and USPS datasets.\n",
    "\n",
    "Of these datasets pendigits is the smallest, with only 1797 samples, and is only 64 dimensional. This makes a good first dataset to test things out on -- the dataset is small enough that practically anything should be able to run on this efficiently.\n",
    "\n",
    "USPS provides a slightly more challenging dataset, with almost 10,000 samples and 256 dimensions, but is still samall enough to be tractable for even naive clustering implementations.\n",
    "\n",
    "MNIST provides a good basic scaling test with 70,000 samples in 784 dimensions. In practice this is not a very large dataset compared to many that people want to cluster, although the dimensionality may provide some challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db372ef7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clustering metric eval\n",
    "\n",
    "To make things easier later we will write some short functions to evaluate clusterings (with some special handling of singleton clusters or noise points for clusterign algorithms that support such things), and to plot the results for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a4cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_clusters(cluster_labels0, true_labels, raw_data, cluster_method=\"None\", min_cluster_size=1):\n",
    "    cluster_labels = cluster_labels0.copy()\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    cluster_sizes, size_ids = np.histogram(cluster_labels, bins=unique_labels)\n",
    "    if np.any(cluster_sizes <= min_cluster_size): # Has singleton clusters -- call them noise\n",
    "        singleton_clusters = size_ids[:-1][cluster_sizes < min_cluster_size]\n",
    "        for c in singleton_clusters:\n",
    "            cluster_labels[cluster_labels == c] = -1\n",
    "    if (np.any(cluster_labels < 0)): # Has noise points\n",
    "        clustered_points = (cluster_labels >= 0)\n",
    "        ari = adjusted_rand_score(true_labels[clustered_points], cluster_labels[clustered_points])\n",
    "        ami = adjusted_mutual_info_score(true_labels[clustered_points], cluster_labels[clustered_points])\n",
    "        sil = silhouette_score(raw_data[clustered_points], cluster_labels[clustered_points])\n",
    "        pct_clustered = (np.sum(clustered_points) / cluster_labels.shape[0])\n",
    "        #print(f\"ARI: {ari:.4f}\\nAMI: {ami:.4f}\\nSilhouette: {sil:.4f}\\nPct clustered: {pct_clustered * 100:.2f}%\")\n",
    "    else:\n",
    "        ari = adjusted_rand_score(true_labels, cluster_labels)\n",
    "        ami = adjusted_mutual_info_score(true_labels, cluster_labels)\n",
    "        sil = silhouette_score(raw_data, cluster_labels)\n",
    "        #print(f\"ARI: {ari:.4f}\\nAMI: {ami:.4f}\\nSilhouette: {sil:.4f}\")\n",
    "        pct_clustered = 1.0\n",
    "    \n",
    "    return {\"Method\": cluster_method, \"ARI\": ari, \"AMI\": ami, \"Silhouette\": sil, \"Pct Clustered\": pct_clustered}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ada6e3-a261-466f-87db-5da276d41677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(results_dataframe, score_types=(\"ARI\", \"AMI\"), colors=list(sns.color_palette()), width=0.75, by='Dim Reduction', baselines=None):\n",
    "    if(baselines is not None):\n",
    "        if(len(score_types)!=len(baselines)):\n",
    "            raise ValueError('Need to same length for score_types and baselines')\n",
    "    else:\n",
    "        baselines = [0]*len(score_types)\n",
    "    fig, axs = plt.subplots(1, len(score_types), figsize=(8 * len(score_types), 8))\n",
    "    x_ticklabels = results_dataframe.Method.unique()\n",
    "    x_positions = np.arange(len(x_ticklabels), dtype=np.float32) - width / 2\n",
    "    dim_red_types = results_dataframe[by].unique()\n",
    "    bar_width = width / len(dim_red_types)\n",
    "    for offset_idx, dim_red in enumerate(dim_red_types):\n",
    "        color = colors[offset_idx]\n",
    "        for i, score_type in enumerate(score_types):\n",
    "            sub_dataframe = results_dataframe[\n",
    "                (results_dataframe[\"Score Type\"] == score_type) &\n",
    "                (results_dataframe[by] == dim_red)\n",
    "            ]\n",
    "            nb_method = sub_dataframe.shape[0]\n",
    "            axs[i].bar(\n",
    "                x=x_positions[:nb_method],\n",
    "                height=sub_dataframe[\"Score\"],\n",
    "                width=bar_width,\n",
    "                align=\"edge\",\n",
    "                color=[(*color, v) for v in sub_dataframe[\"Pct Clustered\"]],\n",
    "                label=dim_red if i ==0 else None,\n",
    "            )\n",
    "            axs[i].set_xlabel(\"Cluster Method\")\n",
    "            axs[i].set_xticks(np.arange(len(x_ticklabels)))\n",
    "            axs[i].set_xticklabels(x_ticklabels)\n",
    "            axs[i].set_ylabel(f\"{score_type} Score\")\n",
    "            axs[i].set_title(score_type, fontsize=20)\n",
    "            axs[i].grid(visible=False, axis=\"x\")\n",
    "            axs[i].set_ylim([0, 1.05])\n",
    "        x_positions[:nb_method] = x_positions[:nb_method] + bar_width\n",
    "        x_positions[nb_method:] = x_positions[nb_method:] + 0.5*bar_width\n",
    "    for i in range(len(score_types)):\n",
    "        axs[i].axhline(baselines[i])\n",
    "        \n",
    "    if len(dim_red_types) > 1:\n",
    "        fig.legend(loc=\"center right\", bbox_to_anchor=(1.125, 0.5), borderaxespad=0.0, fontsize=20)\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2478e1dc-35f0-46a6-a160-b08e2abf9708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores_over_x(results_dataframe, x, y, hue, plot_types_column ='Score Type' ,plot_types=(\"ARI\", \"AMI\"), colors=list(sns.color_palette()), baselines=None):\n",
    "    if(baselines is not None):\n",
    "        if(len(plot_types)!=len(baselines)):\n",
    "            raise ValueError('Need to same length for plot_types and baselines')\n",
    "    else:\n",
    "        baselines = [0]*len(plot_types)\n",
    "    fig, axs = plt.subplots(1, len(plot_types), figsize=(8 * len(plot_types), 8))\n",
    "    \n",
    "    for i in range(len(plot_types)):\n",
    "        df = results_dataframe[results_dataframe[plot_types_column]==plot_types[i]]\n",
    "        axs[i].set_ylim([0, 1.05])\n",
    "        sns.barplot(data=df, x=x, y=y, hue=hue, ax=axs[i])\n",
    "        axs[i].axhline(baselines[i])\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eab6175-5373-4898-9254-e20535b6a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph_clustering_algorithm(algo_list, G, weight = 'weight'):\n",
    "    clusterings = dict()\n",
    "    for algo in algo_list:\n",
    "        #print(f\"Running {algo}...\")\n",
    "        if algo == 'Louvain':\n",
    "            clusterings[algo] = G.community_multilevel()\n",
    "        elif algo == 'Louvain \\n+ weight':\n",
    "            clusterings[algo] = G.community_multilevel(weights = weight)\n",
    "        elif algo == 'Label \\nPropagation':\n",
    "            clusterings[algo] = G.community_label_propagation()\n",
    "        elif algo == 'ECG':\n",
    "            clusterings[algo] = G.community_ecg()\n",
    "        elif algo == 'Leiden':\n",
    "            clusterings[algo] = la.find_partition(G, la.ModularityVertexPartition)\n",
    "        elif algo == 'Leiden \\n+ weight':\n",
    "            clusterings[algo] = la.find_partition(G, la.ModularityVertexPartition, weights=weight)\n",
    "            \n",
    "        cluster_labels = {algo:np.array(cluster.membership) for algo, cluster in clusterings.items()}\n",
    "    return(clusterings, cluster_labels)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7558e642-a8ed-44fd-b79b-69583a8e4363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_dataframe(cluster_labels, true_labels, raw_data, min_cluster_size=1):\n",
    "    eval_results = pd.DataFrame(\n",
    "        [\n",
    "            eval_clusters(algo_labels, true_labels, raw_data, cluster_method=algo, min_cluster_size=min_cluster_size)\n",
    "            for algo, algo_labels in cluster_labels.items() \n",
    "        ]\n",
    "    )\n",
    "    return(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9aad4b4-0ab7-4b1d-86ad-03f44f6fead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_non_noise(clustering_labels_dict, skip_eval):\n",
    "    clustering_noise = dict()\n",
    "    for name, labels in clustering_labels_dict.items():\n",
    "        new_labels = labels.copy()\n",
    "        new_labels[skip_eval] = -1\n",
    "        clustering_noise[name] = new_labels\n",
    "    return(clustering_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0512582c-dac1-401b-ae57-1179f7f20d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(cluster_labels, G):\n",
    "    for method, cluster in cluster_labels.items():\n",
    "        print(f\"{method}: \\n\\tModularity: {G.modularity(cluster):.2f}, \\n\\tPartition size: {1+max(cluster)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d9ecc47-696c-4156-86a4-8c0edf8e5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list = ['Louvain', 'Louvain \\n+ weight', 'Label \\nPropagation', 'ECG', 'Leiden', 'Leiden \\n+ weight']\n",
    "algo_list2 = ['Louvain',  'Louvain \\n+ weight', 'Leiden', 'Leiden \\n+ weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5176aba-1a1b-4485-8ff8-34f0f230104c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compare graph algorithms performance on different graph constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e934e07-20d2-4700-9b96-559c9351f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_clustering_on_graph_variants(dataset_id):\n",
    "    raw_data, targets, dataset_name = get_dataset(dataset_id)\n",
    "    display(Markdown(f'## {dataset_name}'))\n",
    "\n",
    "    umap_rep = get_umap_vectors(dataset_id=dataset_id, raw_data=raw_data)\n",
    "    hd_umap_labels = h_dbscan(umap_rep, which_algo='hdbscan', dataset_id=dataset_id)\n",
    "    ari = adjusted_rand_score(targets, hd_umap_labels)\n",
    "    ami = adjusted_mutual_info_score(targets, hd_umap_labels)\n",
    "    print(f'{sum(hd_umap_labels<0)} noise vertices according to HDBSCAN')\n",
    "\n",
    "    results = []\n",
    "    # Graph from fuzzy union\n",
    "    G = get_umap_graph(raw_data, dataset_id=dataset_id, set_op_mix_ratio=1)\n",
    "    clusterings, clustering_labels  = run_graph_clustering_algorithm(algo_list, G)\n",
    "    graph_results = evaluation_dataframe(clustering_labels, targets, raw_data, min_cluster_size=5)\n",
    "    graph_results_union = graph_results.melt([\"Method\", \"Pct Clustered\"], var_name=\"Score Type\", value_name=\"Score\")\n",
    "    graph_results_union[\"Graph\"] = 'Fuzzy Union'\n",
    "    results.append(graph_results_union)\n",
    "\n",
    "    # Graph union no noise vertex\n",
    "    G_sub = G.copy()\n",
    "    G_sub.es.select(_incident = np.where(hd_umap_labels<0)[0]).delete()\n",
    "    clusterings, clustering_labels  = run_graph_clustering_algorithm(algo_list, G_sub)\n",
    "    graph_results = evaluation_dataframe(clustering_labels, targets, raw_data, min_cluster_size=5)\n",
    "    graph_results_nonoise = graph_results.melt([\"Method\", \"Pct Clustered\"], var_name=\"Score Type\", value_name=\"Score\")\n",
    "    graph_results_nonoise[\"Graph\"] = 'No noise vertex'\n",
    "    results.append(graph_results_nonoise)\n",
    "\n",
    "    # Graph from fuzzy intersection\n",
    "    G = get_umap_graph(raw_data, dataset_id=dataset_id, set_op_mix_ratio=0)\n",
    "    clusterings, clustering_labels  = run_graph_clustering_algorithm(algo_list, G)\n",
    "    graph_results = evaluation_dataframe(clustering_labels, targets, raw_data, min_cluster_size=5)\n",
    "    graph_results_inter = graph_results.melt([\"Method\", \"Pct Clustered\"], var_name=\"Score Type\", value_name=\"Score\")\n",
    "    graph_results_inter[\"Graph\"] = 'Fuzzy Intersection'\n",
    "    results.append(graph_results_inter)\n",
    "\n",
    "    params = get_dataset_params(dataset_id)\n",
    "    # Graph from low dimension\n",
    "    G = get_umap_graph(umap_rep, dataset_id=dataset_id, set_op_mix_ratio=1)\n",
    "    clusterings, clustering_labels  = run_graph_clustering_algorithm(algo_list, G)\n",
    "    graph_results = evaluation_dataframe(clustering_labels, targets, raw_data, min_cluster_size=5)\n",
    "    graph_results_low = graph_results.melt([\"Method\", \"Pct Clustered\"], var_name=\"Score Type\", value_name=\"Score\")\n",
    "    graph_results_low[\"Graph\"] = 'Low dimension'\n",
    "    results.append(graph_results_low)\n",
    "\n",
    "    results_df = pd.concat(results)\n",
    "\n",
    "    plot_scores(results_df, by='Graph', baselines=[ari,ami])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f126c15-75c5-4502-841e-b328c50baa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for i in range(6):\n",
    "    graph_clustering_on_graph_variants(dataset_id=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717af1d5-f792-4c2c-af75-2bc6c144defe",
   "metadata": {},
   "source": [
    "# Vary k for graph clustering..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f39a153-1f09-4e14-b602-607c4cd5579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list3 = ['Leiden', 'Leiden \\n+ weight']\n",
    "def varying_k(dataset_id):\n",
    "    raw_data, targets, dataset_name = get_dataset(dataset_id)\n",
    "    display(Markdown(f'## {dataset_name}'))    \n",
    "\n",
    "    umap_rep = get_umap_vectors(dataset_id=dataset_id, raw_data=raw_data)\n",
    "    hd_umap_labels = h_dbscan(umap_rep, which_algo='hdbscan', dataset_id=dataset_id)\n",
    "    ari = adjusted_rand_score(targets, hd_umap_labels)\n",
    "    ami = adjusted_mutual_info_score(targets, hd_umap_labels)\n",
    "\n",
    "    params = get_dataset_params(dataset_id).copy()\n",
    "    graph_results = []\n",
    "    for k in [3, 5, 8, 12, 20, 25, 30, 40]:\n",
    "        params['n_neighbors'] = k\n",
    "        G = get_umap_graph(raw_data, dataset_id=dataset_id, set_op_mix_ratio=1.0, params=params)\n",
    "        G = graph_edge_class_from_labels(G, targets, attribute_name = 'internal')\n",
    "        clusterings, clustering_labels  = run_graph_clustering_algorithm(algo_list3, G)\n",
    "        graph_results_part = evaluation_dataframe(clustering_labels, targets, raw_data, min_cluster_size=5)\n",
    "        graph_results_part['# NNs'] = params['n_neighbors']\n",
    "        graph_results.append(graph_results_part)\n",
    "\n",
    "    umap_results = pd.concat(graph_results)\n",
    "    umap_results_long = umap_results.melt([\"Method\", \"Pct Clustered\", '# NNs'], var_name=\"Score Type\", value_name=\"Score\")\n",
    "    plot_scores_over_x(umap_results_long, x='# NNs', y='Score', hue='Method', baselines=[ari, ami])\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22058fa-f1b7-47c0-a800-f31cd3a1fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "for i in range(6):\n",
    "    varying_k(dataset_id=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6b3eae-173b-4ef3-bd17-09ebe359c6e1",
   "metadata": {},
   "source": [
    "# Timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8248760-4f38-4cdd-bf25-14b4a94916e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b357d-190d-4489-beff-18a26f52fa0b",
   "metadata": {},
   "source": [
    "## On MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84ff7081-44e4-4bdc-a48d-f87728947ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = fetch_openml(\"MNIST_784\")\n",
    "raw_data = np.asarray(mnist.data.astype(np.float32))\n",
    "targets = np.array(mnist.target.astype('int'))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f524356-82ea-4fc6-b70c-04517e55ad0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 52.44334673881531 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "umap_rep = get_umap_vectors(dataset_id=2, raw_data=raw_data)\n",
    "hd_umap_labels = h_dbscan(umap_rep, which_algo='hdbscan', dataset_id=2)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b892b52-ed4a-4dac-9401-746b3695058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 11.427457571029663 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "G = get_umap_graph(raw_data, dataset_id=2, set_op_mix_ratio=1.0)\n",
    "leiden_labels = la.find_partition(G, la.ModularityVertexPartition).membership\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3bc075a0-b0b7-47c1-978e-8ab666c794b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7958436475796535\n"
     ]
    }
   ],
   "source": [
    "print(adjusted_rand_score(leiden_labels, hd_umap_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e75e18b-2488-4cd6-aa63-fd52bf8f61e6",
   "metadata": {},
   "source": [
    "## On CovType data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c8ed49a-ad73-476b-9be4-2cdbaddd29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6370718f-8896-44ad-91bd-eaa3507ddf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 14:21:02,773 - _covtype - INFO - Downloading https://ndownloader.figshare.com/files/5976039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(581012, 54)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covtype = fetch_covtype() \n",
    "raw_data = np.asarray(covtype.data.astype(np.float32))\n",
    "targets = np.array(covtype.target.astype('int'))\n",
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c2255c6-15b2-4ffe-aa98-badd40ad92e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1386.5317125320435 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "umap_rep = umap.UMAP(n_neighbors=15,\n",
    "                         n_components=8, \n",
    "                         min_dist=0.0001, \n",
    "                         random_state=42).fit(raw_data).embedding_\n",
    "hd_umap_labels = hdbscan.HDBSCAN(min_samples=10, \n",
    "                                     min_cluster_size=100).fit_predict(umap_rep)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf22ddf2-c1e2-47d2-9516-beb2e747bb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 61.105358362197876 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "A, sigmas, rhos, dists = umap.umap_.fuzzy_simplicial_set(X=raw_data, \n",
    "                                                 n_neighbors=10, \n",
    "                                                 random_state=42, \n",
    "                                                 metric='euclidean', \n",
    "                                                 return_dists=True,\n",
    "                                                set_op_mix_ratio=1.0)\n",
    "G = ig.Graph.Weighted_Adjacency(A, 'undirected')\n",
    "leiden_labels = la.find_partition(G, la.ModularityVertexPartition).membership\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0104666b-08fc-4b23-81b5-e7b29b5600e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.721311475409838"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1386/61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef6ea2f2-3a25-46fa-af64-bb2f0afe3847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020146426580477987\n"
     ]
    }
   ],
   "source": [
    "print(adjusted_rand_score(leiden_labels, hd_umap_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bda524-6ce0-4fd9-8209-c1c283610fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HighDimensionalClustering",
   "language": "python",
   "name": "highdimensionalclustering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
