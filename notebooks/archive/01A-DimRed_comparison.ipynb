{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e429df",
   "metadata": {},
   "source": [
    "# Clustering evaluation on high dimensional data\n",
    "\n",
    "Starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4875757-c802-423c-8bf8-8efd45ff78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \u001b[32mmain\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d3bdf4-9b88-4008-93e6-5e6f656e5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a84a75d1-6a4c-4246-ac21-87fb0b1d7deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pendigits', 'coil', 'mnist', 'usps', 'buildings', 'clusterable']\n"
     ]
    }
   ],
   "source": [
    "execfile('functions/data_specifics.py')\n",
    "execfile('functions/graph_functions.py')\n",
    "print(data_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a03c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import cluster\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import imageio\n",
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import hdbscan\n",
    "import umap\n",
    "from sklearn.neighbors import KNeighborsTransformer\n",
    "import pynndescent\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db372ef7",
   "metadata": {},
   "source": [
    "# Clustering metric eval\n",
    "\n",
    "To make things easier later we will write some short functions to evaluate clusterings (with some special handling of singleton clusters or noise points for clusterign algorithms that support such things), and to plot the results for easy comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6a4cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_clusters(cluster_labels, true_labels, raw_data, cluster_method=\"None\", min_cluster_size=5):\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    cluster_sizes, size_ids = np.histogram(cluster_labels, bins=unique_labels)\n",
    "    if np.any(cluster_sizes == 1): # Has singleton clusters -- call them noise\n",
    "        singleton_clusters = size_ids[:-1][cluster_sizes <= min_cluster_size]\n",
    "        for c in singleton_clusters:\n",
    "            cluster_labels[cluster_labels == c] = -1\n",
    "    if np.any(cluster_labels < 0): # Has noise points\n",
    "        clustered_points = (cluster_labels >= 0)\n",
    "        ari = adjusted_rand_score(true_labels[clustered_points], cluster_labels[clustered_points])\n",
    "        ami = adjusted_mutual_info_score(true_labels[clustered_points], cluster_labels[clustered_points])\n",
    "        sil = silhouette_score(raw_data[clustered_points], cluster_labels[clustered_points])\n",
    "        pct_clustered = (np.sum(clustered_points) / cluster_labels.shape[0])\n",
    "        # print(f\"ARI: {ari:.4f}\\nAMI: {ami:.4f}\\nSilhouette: {sil:.4f}\\nPct clustered: {pct_clustered * 100:.2f}%\")\n",
    "    else:\n",
    "        ari = adjusted_rand_score(true_labels, cluster_labels)\n",
    "        ami = adjusted_mutual_info_score(true_labels, cluster_labels)\n",
    "        sil = silhouette_score(raw_data, cluster_labels)\n",
    "        # print(f\"ARI: {ari:.4f}\\nAMI: {ami:.4f}\\nSilhouette: {sil:.4f}\")\n",
    "        pct_clustered = 1.0\n",
    "    \n",
    "    return {\"Method\": cluster_method, \"ARI\": ari, \"AMI\": ami, \"Silhouette\": sil, \"Pct Clustered\": pct_clustered}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a555f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(results_dataframe, score_types=(\"ARI\", \"AMI\"), colors=list(sns.color_palette()), width=0.75):\n",
    "    fig, axs = plt.subplots(1, len(score_types), figsize=(8 * len(score_types), 8))\n",
    "    x_ticklabels = results_dataframe.Method.unique()\n",
    "    x_positions = np.arange(len(x_ticklabels), dtype=np.float32) - width / 2\n",
    "    dim_red_types = results_dataframe[\"Dim Reduction\"].unique()\n",
    "    bar_width = width / len(dim_red_types)\n",
    "    for offset_idx, dim_red in enumerate(dim_red_types):\n",
    "        color = colors[offset_idx]\n",
    "        for i, score_type in enumerate(score_types):\n",
    "            sub_dataframe = results_dataframe[\n",
    "                (results_dataframe[\"Score Type\"] == score_type) &\n",
    "                (results_dataframe[\"Dim Reduction\"] == dim_red)\n",
    "            ]\n",
    "            axs[i].bar(\n",
    "                x=x_positions,\n",
    "                height=sub_dataframe[\"Score\"],\n",
    "                width=bar_width,\n",
    "                align=\"edge\",\n",
    "                color=[(*color, v) for v in sub_dataframe[\"Pct Clustered\"]],\n",
    "                label=dim_red if i ==0 else None,\n",
    "            )\n",
    "            axs[i].set_xlabel(\"Cluster Method\")\n",
    "            axs[i].set_xticks(np.arange(len(x_ticklabels)))\n",
    "            axs[i].set_xticklabels(x_ticklabels)\n",
    "            axs[i].set_ylabel(f\"{score_type} Score\")\n",
    "            axs[i].set_title(score_type, fontsize=20)\n",
    "            axs[i].grid(visible=False, axis=\"x\")\n",
    "            axs[i].set_ylim([0, 1.05])\n",
    "        x_positions += bar_width\n",
    "        \n",
    "    if len(dim_red_types) > 1:\n",
    "        fig.legend(loc=\"center right\", bbox_to_anchor=(1.125, 0.5), borderaxespad=0.0, fontsize=20)\n",
    "        \n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478e2414-cd1d-4af2-8962-5a1c109a6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clustering_algorithms(raw_data, targets, list_clustering_algo, params, name=None):\n",
    "    results = dict()\n",
    "    if(\"K-Means\" in list_clustering_algo):\n",
    "        results[\"K-Means\"] = cluster.KMeans(n_clusters=max(targets)+1).fit_predict(raw_data)\n",
    "    if(\"Complete\\nLinkage\" in list_clustering_algo):\n",
    "        results[\"Complete\\nLinkage\"] = cluster.AgglomerativeClustering(n_clusters=max(targets)+1, linkage=\"complete\").fit_predict(raw_data)\n",
    "    if(\"Single\\nLinkage\" in list_clustering_algo):\n",
    "        results[\"Single\\nLinkage\"] = cluster.AgglomerativeClustering(n_clusters=params['sl_k'], linkage=\"single\").fit_predict(raw_data)\n",
    "    if(\"DBSCAN\" in list_clustering_algo):\n",
    "        results[\"DBSCAN\"] = cluster.DBSCAN(eps=params['dbscan_eps']).fit_predict(raw_data)\n",
    "    if(\"HDBSCAN\" in list_clustering_algo):\n",
    "        results[\"HDBSCAN\"] = hdbscan.HDBSCAN(min_samples=params['hdbscan_min_samples'], min_cluster_size=params['hdbscan_min_cluster_size']).fit_predict(raw_data)\n",
    "\n",
    "    raw_results = pd.DataFrame(\n",
    "        [\n",
    "            eval_clusters(algo_labels, targets, raw_data, cluster_method=algo_name) for algo_name, algo_labels in results.items()\n",
    "        ]\n",
    "    )\n",
    "    raw_results_long = raw_results.melt([\"Method\", \"Pct Clustered\"], var_name=\"Score Type\", value_name=\"Score\")\n",
    "    raw_results_long[\"Dim Reduction\"] = str(name)\n",
    "    return(raw_results_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c50b5a88-3dc9-40f5-856e-f6c1732f7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_analysis(i, list_clustering_algo = [\"K-Means\", \"Single\\nLinkage\", \"HDBSCAN\"]):   \n",
    "    raw_data, targets, dataset_name = get_dataset(dataset_id = i)\n",
    "    display(Markdown(f'## {dataset_name}'))\n",
    "\n",
    "    params = dataset_clustering_params['None'][i]\n",
    "    raw_results_long = run_clustering_algorithms(raw_data, targets, list_clustering_algo, params)\n",
    "\n",
    "    params = dataset_clustering_params['PCA'][i]\n",
    "    pca_data = PCA(n_components=params['pca_n_components']).fit_transform(raw_data)\n",
    "    pca_results_long = run_clustering_algorithms(pca_data, targets, list_clustering_algo, params, name='PCA')\n",
    "\n",
    "    params = dataset_clustering_params['UMAP'][i]\n",
    "    umap_data = get_umap_vectors(dataset_id=i)\n",
    "    umap_results_long = run_clustering_algorithms(umap_data, targets, list_clustering_algo, params, name='UMAP')\n",
    "\n",
    "    return(pd.concat([raw_results_long, pca_results_long, umap_results_long]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ba3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## pendigits"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## coil"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/manifold/_spectral_embedding.py:274: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## mnist"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## usps"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## buildings"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/disk/home/vmpouli/.conda/envs/tnt_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = dict()\n",
    "for i in range(5):\n",
    "    results[data_set_list[i]] = run_analysis(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ddd826-bdbb-4561-bb31-7afa1331ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    display(Markdown(f'## {data_set_list[i]}'))\n",
    "    plot_scores(results[data_set_list[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnt_env",
   "language": "python",
   "name": "tnt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
