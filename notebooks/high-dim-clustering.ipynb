{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed413bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "import sklearn.datasets\n",
    "import umap\n",
    "import pynndescent\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy.sparse\n",
    "import sklearn.metrics\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(12, 12)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb619dae-1629-4955-b637-ff7aac095bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = sklearn.datasets.fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116c1b5",
   "metadata": {},
   "source": [
    "## Build the Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eca59c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the models (using V(X) = E(X^2) - E(X)^2 so we can stream through the data)\n",
    "# Quirk: We use the 2-out neighborhood and accept the double counting of neighbors of neightbors\n",
    "#        that overlap. The 2-out lets us see further; the double counting biases toward \n",
    "#        the coherent local region as opposed to outliers. Also it just works better. Better\n",
    "#        options or justifications are more than welcome.\n",
    "@numba.njit(fastmath=True)\n",
    "def build_models(nn_inds, nn_dists):\n",
    "    result = np.zeros((nn_inds.shape[0], 5), dtype=np.float32)\n",
    "    sums = np.zeros(nn_inds.shape[0], dtype=np.float32)\n",
    "    sums_of_squares = np.zeros(nn_inds.shape[0], dtype=np.float32)\n",
    "    counts = np.zeros(nn_inds.shape[0], dtype=np.float32)\n",
    "    # Get sums and counts for the 1-out (not including the points own 1-nn dist)\n",
    "    for i in range(nn_inds.shape[0]):\n",
    "        for j in range(1, nn_inds.shape[1]):\n",
    "            k = nn_inds[i, j]\n",
    "            if k != i and nn_dists[k] > 0.0: # Skip zero dists since they don't fit the model\n",
    "                d = nn_dists[k]\n",
    "                sums[i] += d\n",
    "                sums_of_squares[i] += d * d\n",
    "                counts[i] += 1.0\n",
    "                \n",
    "    # Total up totals for the 2-out then compute the mean and std\n",
    "    for i in range(nn_inds.shape[0]):\n",
    "        count = 0\n",
    "        for j in range(nn_inds.shape[1]):\n",
    "            k = nn_inds[i, j]\n",
    "            result[i, 0] += sums[k]\n",
    "            result[i, 1] += sums_of_squares[k]\n",
    "            count += counts[k]\n",
    "            \n",
    "        result[i, 0] /= count\n",
    "        result[i, 1] = np.sqrt(result[i, 1] / count - result[i, 0] ** 2)\n",
    "    result[:, 2] = sums\n",
    "    result[:, 3] = sums_of_squares\n",
    "    result[:, 4] = counts\n",
    "    \n",
    "    return result\n",
    "    \n",
    "# Create an edge list of the (directed!) prob of being a nearest neighbor\n",
    "# This amounts to just computing the relevant prob of the relevant normal using erfc from\n",
    "# the math library.\n",
    "@numba.njit(fastmath=True)\n",
    "def build_edges(nn_inds, nn_dists, models, max_total_weight=32.0, min_prob=1e-3):\n",
    "    result = []\n",
    "    root_two = np.sqrt(2)\n",
    "    for i in range(nn_inds.shape[0]):\n",
    "        mean = models[i, 0]\n",
    "        std = models[i, 1]\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        for j in range(nn_inds.shape[1]):\n",
    "            k = nn_inds[i, j]\n",
    "            if nn_dists[i, j] == 0:\n",
    "                if i != k:\n",
    "                    result.append((i, k, 1.0))\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            d = nn_dists[i, j]\n",
    "            if std > 0:\n",
    "                erfc_input = (d - mean) / (root_two * std)\n",
    "                val = math.erfc(erfc_input) / 2.0\n",
    "            else:\n",
    "                val = 0.0\n",
    "            \n",
    "\n",
    "            total_weight += val\n",
    "            if total_weight > max_total_weight or val < min_prob:\n",
    "                break\n",
    "            else:\n",
    "                result.append((i, k, val))\n",
    "            \n",
    "    return result\n",
    "\n",
    "# Build the graph: compute nearest neighbors (approximately), build models from them, then make an edge\n",
    "# list and convert that into a scipy sparse matrix.\n",
    "def construct_prob_graph(data, n_neighbors=30, metric=\"euclidean\", max_total_weight=32.0, min_prob=1e-3, k=1):\n",
    "    nn_index = pynndescent.NNDescent(data, metric=metric, n_neighbors=2 * n_neighbors)\n",
    "    nn_inds, nn_dists = nn_index.neighbor_graph\n",
    "    models = build_models(nn_inds, nn_dists[:, k])\n",
    "    \n",
    "    graph_edges = np.asarray(\n",
    "        build_edges(nn_inds, nn_dists, models, max_total_weight=max_total_weight, min_prob=min_prob)\n",
    "    )\n",
    "    result = scipy.sparse.coo_matrix(\n",
    "        (graph_edges.T[2], (graph_edges.T[0].astype(np.int32), graph_edges.T[1].astype(np.int32))),\n",
    "        shape=(data.shape[0], data.shape[0])\n",
    "    )\n",
    "    result.eliminate_zeros()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9616bbf",
   "metadata": {},
   "source": [
    "## Cluster the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6585c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "RankDisjointSet = namedtuple(\"DisjointSet\", [\"parent\", \"rank\"])\n",
    "SizeDisjointSet = namedtuple(\"DisjointSet\", [\"parent\", \"size\"])\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_rank_create(n_elements):\n",
    "    return RankDisjointSet(np.arange(n_elements, dtype=np.int32), np.zeros(n_elements, dtype=np.int32))\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_size_create(n_elements):\n",
    "    return SizeDisjointSet(np.arange(n_elements, dtype=np.int32), np.ones(n_elements, dtype=np.int32))\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_find(disjoint_set, x):\n",
    "    while disjoint_set.parent[x] != x:\n",
    "        x, disjoint_set.parent[x] = disjoint_set.parent[x], disjoint_set.parent[disjoint_set.parent[x]]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_union_by_rank(disjoint_set, x, y):\n",
    "    x = ds_find(disjoint_set, x)\n",
    "    y = ds_find(disjoint_set, y)\n",
    "\n",
    "    if x == y:\n",
    "        return\n",
    "\n",
    "    if disjoint_set.rank[x] < disjoint_set.rank[y]:\n",
    "        x, y = y, x\n",
    "\n",
    "    disjoint_set.parent[y] = x\n",
    "    if disjoint_set.rank[x] == disjoint_set.rank[y]:\n",
    "        disjoint_set.rank[x] += 1\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_union_by_size(disjoint_set, x, y):\n",
    "    x = ds_find(disjoint_set, x)\n",
    "    y = ds_find(disjoint_set, y)\n",
    "\n",
    "    if x == y:\n",
    "        return\n",
    "\n",
    "    if disjoint_set.size[x] < disjoint_set.size[y]:\n",
    "        x, y = y, x\n",
    "\n",
    "    disjoint_set.parent[y] = x\n",
    "    disjoint_set.size[x] += disjoint_set.size[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd833b6",
   "metadata": {},
   "source": [
    "Next we need single linkage clustering of the graph. That is most easily done by computing a minimum spanning forest, and then processing that into a forest of merge trees. We could use scipy here, but we want flexibility (for forests instead of trees), and they just use Kruskal's algorithm anyway, which is easy to reproduce here. After that we just need the standard process for converting a spanning tree (or forest) to a merge tree (or forest). In practice I think this can all be simplified to one function, since we are doing the merge work in Kruskal's but for now let's keep things as separate standard algorithms.\n",
    "\n",
    "Note that to use minimum spanning forests, and merge order structures we need things going in the other order than probabilities, so we just use negative logs (after all, it's only order that matters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d6afed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def kruskal_minimum_spanning_forest(rows, cols, data, n_vertices):\n",
    "    \n",
    "    result_row = np.zeros(n_vertices - 1, dtype=np.int64)\n",
    "    result_col = np.zeros(n_vertices - 1, dtype=np.int64)\n",
    "    result_data = np.zeros(n_vertices - 1, dtype=np.float32)\n",
    "    mst_edge_idx = 0\n",
    "        \n",
    "    edge_order = np.argsort(data)\n",
    "    disjoint_set = ds_rank_create(n_vertices)\n",
    "    \n",
    "    for n in range(data.shape[0]):\n",
    "        edge_idx = edge_order[n]\n",
    "        i = rows[edge_idx]\n",
    "        j = cols[edge_idx]\n",
    "        \n",
    "        i_component = ds_find(disjoint_set, i)\n",
    "        j_component = ds_find(disjoint_set, j)\n",
    "        \n",
    "        if i_component != j_component:\n",
    "            result_row[mst_edge_idx] = i\n",
    "            result_col[mst_edge_idx] = j\n",
    "            result_data[mst_edge_idx] = data[edge_idx]\n",
    "            mst_edge_idx += 1\n",
    "            ds_union_by_rank(disjoint_set, i, j)\n",
    "            \n",
    "            if mst_edge_idx >= n_vertices - 1:\n",
    "                break\n",
    "            \n",
    "    return result_row[:mst_edge_idx], result_col[:mst_edge_idx], result_data[:mst_edge_idx]\n",
    "\n",
    "LinkageMergeData = namedtuple(\"LinkageMergeData\", [\"parent\", \"size\", \"next\"])\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def create_linkage_merge_data(base_size):\n",
    "    parent = np.full(2 * base_size - 1, -1, dtype=np.intp)\n",
    "    size = np.concatenate((np.ones(base_size, dtype=np.intp), np.zeros(base_size - 1, dtype=np.intp)))\n",
    "    next_parent = np.array([base_size], dtype=np.intp)\n",
    "\n",
    "    return LinkageMergeData(parent, size, next_parent)\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def linkage_merge_find(linkage_merge, node):\n",
    "    relabel = node\n",
    "    while linkage_merge.parent[node] != -1 and linkage_merge.parent[node] != node:\n",
    "        node = linkage_merge.parent[node]\n",
    "\n",
    "    linkage_merge.parent[node] = node\n",
    "\n",
    "    # label up to the root\n",
    "    while linkage_merge.parent[relabel] != node:\n",
    "        next_relabel = linkage_merge.parent[relabel]\n",
    "        linkage_merge.parent[relabel] = node\n",
    "        relabel = next_relabel\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def linkage_merge_join(linkage_merge, left, right):\n",
    "    linkage_merge.size[linkage_merge.next[0]] = linkage_merge.size[left] + linkage_merge.size[right]\n",
    "    linkage_merge.parent[left] = linkage_merge.next[0]\n",
    "    linkage_merge.parent[right] = linkage_merge.next[0]\n",
    "    linkage_merge.next[0] += 1\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def msf_to_linkage_forest(sorted_mst, n_samples=-1):\n",
    "    result = np.empty((sorted_mst.shape[0], sorted_mst.shape[1] + 1))\n",
    "\n",
    "    if n_samples < 0:\n",
    "        n_samples = sorted_mst.shape[0] + 1\n",
    "        \n",
    "    linkage_merge = create_linkage_merge_data(n_samples)\n",
    "\n",
    "    for index in range(sorted_mst.shape[0]):\n",
    "\n",
    "        left = np.intp(sorted_mst[index, 0])\n",
    "        right = np.intp(sorted_mst[index, 1])\n",
    "        delta = sorted_mst[index, 2]\n",
    "\n",
    "        left_component = linkage_merge_find(linkage_merge, left)\n",
    "        right_component = linkage_merge_find(linkage_merge, right)\n",
    "\n",
    "        if left_component > right_component:\n",
    "            result[index][0] = left_component\n",
    "            result[index][1] = right_component\n",
    "        else:\n",
    "            result[index][1] = left_component\n",
    "            result[index][0] = right_component\n",
    "\n",
    "        result[index][2] = delta\n",
    "        result[index][3] = linkage_merge.size[left_component] + linkage_merge.size[right_component]\n",
    "\n",
    "        linkage_merge_join(linkage_merge, left_component, right_component)\n",
    "\n",
    "    return result\n",
    "\n",
    "def merge_forest_from_graph(prob_graph):\n",
    "    \n",
    "    if scipy.sparse.isspmatrix_coo(prob_graph):\n",
    "        graph_for_linkage = prob_graph\n",
    "    else:\n",
    "        graph_for_linkage = prob_graph.tocoo()\n",
    "        \n",
    "    msf = kruskal_minimum_spanning_forest(\n",
    "        graph_for_linkage.row, graph_for_linkage.col, -np.log(graph_for_linkage.data), graph_for_linkage.shape[0]\n",
    "    )\n",
    "    msf = np.vstack(msf).T\n",
    "    merge_forest = msf_to_linkage_forest(msf, graph_for_linkage.shape[0])\n",
    "    \n",
    "    return merge_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b74bb",
   "metadata": {},
   "source": [
    "Next we need to be able to condense trees, and also forests. We can just use the standard ``condense_tree`` code (see fast_hdbscan) with some minor modifications where the number of points isn't automatically/implicitly encoded in the merge tree structure. We then supplement that with the ability to extract out roots of the merge forest, and then simply run ``condense_tree`` for each tree in our merge forest, and glue the whole thing together when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e0f477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def bfs_from_hierarchy(hierarchy, bfs_root, num_points):\n",
    "    to_process = [bfs_root]\n",
    "    result = []\n",
    "\n",
    "    while to_process:\n",
    "        result.extend(to_process)\n",
    "        next_to_process = []\n",
    "        for n in to_process:\n",
    "            if n >= num_points:\n",
    "                i = n - num_points\n",
    "                next_to_process.append(int(hierarchy[i, 0]))\n",
    "                next_to_process.append(int(hierarchy[i, 1]))\n",
    "        to_process = next_to_process\n",
    "\n",
    "    return result\n",
    "\n",
    "@numba.njit()\n",
    "def eliminate_branch(branch_node, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore, hierarchy,\n",
    "                     num_points):\n",
    "    if branch_node < num_points:\n",
    "        parents[idx] = parent_node\n",
    "        children[idx] = branch_node\n",
    "        lambdas[idx] = lambda_value\n",
    "        idx += 1\n",
    "    else:\n",
    "        for sub_node in bfs_from_hierarchy(hierarchy, branch_node, num_points):\n",
    "            if sub_node < num_points:\n",
    "                children[idx] = sub_node\n",
    "                parents[idx] = parent_node\n",
    "                lambdas[idx] = lambda_value\n",
    "                idx += 1\n",
    "            else:\n",
    "                ignore[sub_node] = True\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "CondensedTree = namedtuple('CondensedTree', ['parent', 'child', 'lambda_val', 'child_size'])\n",
    "\n",
    "@numba.njit()\n",
    "def neg_exp(x):\n",
    "    return np.exp(-x)\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def condense_tree(hierarchy, min_cluster_size=10, root=-1, num_points=-1, next_label=-1, lambda_func=neg_exp):\n",
    "    \n",
    "    if root < 0:\n",
    "        root = 2 * hierarchy.shape[0]\n",
    "        \n",
    "    if num_points < 0:\n",
    "        num_points = hierarchy.shape[0] + 1\n",
    "        \n",
    "    if next_label < 0:\n",
    "        next_label = num_points\n",
    "\n",
    "    node_list = bfs_from_hierarchy(hierarchy, root, num_points)\n",
    "\n",
    "    relabel = np.zeros(root + 1, dtype=np.int64)\n",
    "    relabel[root] = next_label\n",
    "    \n",
    "    next_label += 1\n",
    "\n",
    "    parents = np.ones(root, dtype=np.int64)\n",
    "    children = np.empty(root, dtype=np.int64)\n",
    "    lambdas = np.empty(root, dtype=np.float32)\n",
    "    sizes = np.ones(root, dtype=np.int64)\n",
    "\n",
    "    ignore = np.zeros(root + 1, dtype=np.bool8)\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for node in node_list:\n",
    "        if ignore[node] or node < num_points:\n",
    "            continue\n",
    "\n",
    "        parent_node = relabel[node]\n",
    "        l, r, d, _ = hierarchy[node - num_points]\n",
    "        left = np.int64(l)\n",
    "        right = np.int64(r)\n",
    "        lambda_value = lambda_func(d)\n",
    "\n",
    "        left_count = np.int64(hierarchy[left - num_points, 3]) if left >= num_points else 1\n",
    "        right_count = np.int64(hierarchy[right - num_points, 3]) if right >= num_points else 1\n",
    "\n",
    "        # The logic here is in a strange order, but it has non-trivial performance gains ...\n",
    "        # The most common case by far is a singleton on the left; and cluster on the right take care of this separately\n",
    "        if left < num_points and right_count >= min_cluster_size:\n",
    "            relabel[right] = parent_node\n",
    "            parents[idx] = parent_node\n",
    "            children[idx] = left\n",
    "            lambdas[idx] = lambda_value\n",
    "            idx += 1\n",
    "        # Next most common is a small left cluster and a large right cluster: \n",
    "        #   relabel the right node; eliminate the left branch\n",
    "        elif left_count < min_cluster_size and right_count >= min_cluster_size:\n",
    "            relabel[right] = parent_node\n",
    "            idx = eliminate_branch(left, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore,\n",
    "                                   hierarchy, num_points)\n",
    "        # Then we have a large left cluster and a small right cluster: relabel the left node; elimiate the right branch\n",
    "        elif left_count >= min_cluster_size and right_count < min_cluster_size:\n",
    "            relabel[left] = parent_node\n",
    "            idx = eliminate_branch(right, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore,\n",
    "                                   hierarchy, num_points)\n",
    "        # If both clusters are small then eliminate all branches\n",
    "        elif left_count < min_cluster_size and right_count < min_cluster_size:\n",
    "            idx = eliminate_branch(left, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore,\n",
    "                                   hierarchy, num_points)\n",
    "            idx = eliminate_branch(right, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore,\n",
    "                                   hierarchy, num_points)\n",
    "        # and finally if we actually have a legitimate cluster split, handle that correctly\n",
    "        else:\n",
    "            relabel[left] = next_label\n",
    "\n",
    "            parents[idx] = parent_node\n",
    "            children[idx] = next_label\n",
    "            lambdas[idx] = lambda_value\n",
    "            sizes[idx] = left_count\n",
    "            next_label += 1\n",
    "            idx += 1\n",
    "\n",
    "            relabel[right] = next_label\n",
    "\n",
    "            parents[idx] = parent_node\n",
    "            children[idx] = next_label\n",
    "            lambdas[idx] = lambda_value\n",
    "            sizes[idx] = right_count\n",
    "            next_label += 1\n",
    "            idx += 1\n",
    "\n",
    "    return CondensedTree(parents[:idx], children[:idx], lambdas[:idx], sizes[:idx])\n",
    "\n",
    "@numba.njit()\n",
    "def merge_tree_roots(merge_tree, min_cluster_size, n_samples=-1):\n",
    "    if n_samples < 0:\n",
    "        n_samples = merge_tree.shape[0] + 1\n",
    "        \n",
    "    is_root = np.ones(merge_tree.shape[0], dtype=np.bool_)\n",
    "    for i in range(merge_tree.shape[0]):\n",
    "        if merge_tree[i, 0] >= n_samples:\n",
    "            is_root[int(merge_tree[i, 0] - n_samples)] = False\n",
    "        if merge_tree[i, 1] >= n_samples:\n",
    "            is_root[int(merge_tree[i, 1] - n_samples)] = False\n",
    "        \n",
    "    roots = np.nonzero(is_root)[0] + n_samples\n",
    "    \n",
    "    result = []\n",
    "    for i in range(roots.shape[0]):\n",
    "        if merge_tree[roots[i] - n_samples, 3] > min_cluster_size:\n",
    "            result.append(roots[i])\n",
    "        \n",
    "    return result\n",
    "\n",
    "#@numba.njit()\n",
    "def condense_forest(merge_forest, min_cluster_size=10, n_samples=-1, lambda_func=neg_exp):\n",
    "\n",
    "    if n_samples < 0:\n",
    "        n_samples = merge_tree.shape[0] + 1\n",
    "        \n",
    "    roots = merge_tree_roots(merge_forest, min_cluster_size=min_cluster_size, n_samples=n_samples)\n",
    "    \n",
    "    ctrees = []\n",
    "    next_label = n_samples + 1\n",
    "    for root in roots:\n",
    "        ctree = condense_tree(\n",
    "            merge_forest, \n",
    "            min_cluster_size=min_cluster_size, \n",
    "            root=root, \n",
    "            num_points=n_samples,\n",
    "            next_label=next_label,\n",
    "            lambda_func=lambda_func,\n",
    "        )\n",
    "        next_label = ctree.parent.max() + 1\n",
    "        ctrees.append(ctree)\n",
    "        \n",
    "    result = CondensedTree(\n",
    "        np.concatenate([x.parent for x in ctrees]),\n",
    "        np.concatenate([x.child for x in ctrees]),\n",
    "        np.concatenate([x.lambda_val for x in ctrees]),\n",
    "        np.concatenate([x.child_size for x in ctrees]),\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3c736",
   "metadata": {},
   "source": [
    "Lastly we need to be able to extract clusters. We could use any technique, but I went with the leaf extraction because it seemed to work better. In principle this means we don't need to condense the whole tree, just the leaves which could save a lot of time/trouble, but other extraction methods are possible so I'll stick with flexibility (and borrowed standard algorithms) for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c788e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def extract_leaves(condensed_tree, allow_single_cluster=True):\n",
    "    n_points = condensed_tree.parent.min()\n",
    "    n_nodes = max(condensed_tree.parent.max() + 1, n_points + 1)\n",
    "    leaf_indicator = np.ones(n_nodes, dtype=np.bool_)\n",
    "    leaf_indicator[:n_points] = False\n",
    "\n",
    "    for parent, child_size in zip(condensed_tree.parent, condensed_tree.child_size):\n",
    "        if child_size > 1:\n",
    "            leaf_indicator[parent] = False\n",
    "\n",
    "    return np.nonzero(leaf_indicator)[0]\n",
    "\n",
    "@numba.njit()\n",
    "def get_cluster_label_vector(\n",
    "        tree,\n",
    "        clusters,\n",
    "):\n",
    "    root_cluster = tree.parent.min()\n",
    "    result = np.empty(root_cluster, dtype=np.intp)\n",
    "    cluster_label_map = {c: n for n, c in enumerate(np.sort(clusters))}\n",
    "\n",
    "    disjoint_set = ds_rank_create(tree.parent.max() + 1)\n",
    "    clusters = set(clusters)\n",
    "\n",
    "    for n in range(tree.parent.shape[0]):\n",
    "        child = tree.child[n]\n",
    "        parent = tree.parent[n]\n",
    "        if child not in clusters:\n",
    "            ds_union_by_rank(disjoint_set, parent, child)\n",
    "\n",
    "    for n in range(root_cluster):\n",
    "        cluster = ds_find(disjoint_set, n)\n",
    "        if cluster in cluster_label_map:\n",
    "            result[n] = cluster_label_map[cluster]\n",
    "        else:\n",
    "            result[n] = -1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def labels_from_prob_graph(prob_graph, min_cluster_size):\n",
    "    \n",
    "    merge_forest = merge_forest_from_graph(prob_graph)\n",
    "    condensed_forest = condense_forest(merge_forest, min_cluster_size=min_cluster_size, n_samples=prob_graph.shape[0])\n",
    "    chosen_clusters = extract_leaves(condensed_forest)\n",
    "    \n",
    "    result = get_cluster_label_vector(condensed_forest, chosen_clusters)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9615f47",
   "metadata": {},
   "source": [
    "## Propagate labels through the graph\n",
    "\n",
    "The clustering of the graph works well in that it picks out most of the clusters, but it leaves a great deal of data as noise. We can fix that by running a label propagation through the graph. We do still want to keep the ability to label points as noise, and it would be good to keep the propagation soft, so we can do a Bayesian label propagation of probability vectors over possible labels, including a noise label, with a Bayesian update of the distribution at each propagation round. We can then iterate until relative convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b419bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic Bayesian label prop with a few quirks\n",
    "# Quirks: Different vertices have different (weighted) degrees; we normalize so the total outgoing weight is consistent\n",
    "#         We reassert labels each iteration boosting the prior; this includes a level of noise prior which is a parameter\n",
    "#         This is not really theoretically sound, but it prevents labels over-running where they shouldn't. We could in\n",
    "#         theory simply hold labelled vertics fixed; I haven't experimented with that much yet.\n",
    "@numba.njit(fastmath=True, parallel=True)\n",
    "def bayesian_label_prop_iteration(\n",
    "    indptr, indices, probs, vertex_label_probs, labels, degree_correction, prior=10.0, noise_prior_prob=0.0\n",
    "):\n",
    "    \n",
    "    n_rows = indptr.shape[0] - 1\n",
    "    n_labels = vertex_label_probs.shape[1]\n",
    "    vertex_votes = np.zeros_like(vertex_label_probs, dtype=np.float64)\n",
    "    norms = np.zeros(n_rows)\n",
    "    for i in numba.prange(n_rows):\n",
    "        if labels[i] >= 0:\n",
    "            vertex_votes[i] = prior * vertex_label_probs[i]\n",
    "            norms[i] += prior\n",
    "        else:\n",
    "            vertex_votes[i] = noise_prior_prob * vertex_label_probs[i]\n",
    "            norms[i] += noise_prior_prob\n",
    "#         if labels[i] >= 0:\n",
    "#             vertex_votes[i, labels[i]] = 1.0\n",
    "#             norms[i] += 1.0\n",
    "#         else:\n",
    "#             vertex_votes[i, -1] += noise_prior_prob\n",
    "#             norms[i] += noise_prior_prob\n",
    "        \n",
    "    for i in numba.prange(n_rows):\n",
    "        for k in range(indptr[i], indptr[i+1]):\n",
    "            j = indices[k]\n",
    "            prob = probs[k] * degree_correction[i]\n",
    "            \n",
    "            for l in range(n_labels):\n",
    "                vertex_votes[i, l] += prob * vertex_label_probs[j, l]\n",
    "                \n",
    "            norms[i] += prob\n",
    "        \n",
    "    for i in numba.prange(n_rows):\n",
    "        if norms[i] > 0.0:\n",
    "            vertex_votes[i] /= norms[i]\n",
    "        else:\n",
    "            vertex_votes[i, -1] = 1.0\n",
    "            \n",
    "    return vertex_votes\n",
    "\n",
    "\n",
    "def bayesian_label_prop(prob_graph, label_vector, noise_prior_prob=0.0, max_iter=100, tolerance=1e-5, prior=10.0):\n",
    "    if not scipy.sparse.isspmatrix_csr(prob_graph):\n",
    "        prob_graph = prob_graph.tocsr()\n",
    "        \n",
    "    vertex_label_probs = np.zeros((prob_graph.shape[0], label_vector.max() + 2), dtype=np.float64)\n",
    "    for i in range(vertex_label_probs.shape[0]):\n",
    "        vertex_label_probs[i, label_vector[i]] = 1.0\n",
    "        \n",
    "    degree_correction = 1.0 / (np.squeeze(np.array(prob_graph.sum(axis=0))) + 1e-3)\n",
    "    for i in range(max_iter):\n",
    "        next_vertex_label_probs = bayesian_label_prop_iteration(\n",
    "            prob_graph.indptr, \n",
    "            prob_graph.indices, \n",
    "            prob_graph.data, \n",
    "            vertex_label_probs, \n",
    "            label_vector,\n",
    "            degree_correction,\n",
    "            noise_prior_prob=noise_prior_prob,\n",
    "            prior=prior,\n",
    "        )\n",
    "        \n",
    "        total_change = np.sqrt(np.sum((vertex_label_probs - next_vertex_label_probs)**2))\n",
    "        \n",
    "        if total_change / prob_graph.shape[0] < tolerance:\n",
    "            return next_vertex_label_probs\n",
    "        else:\n",
    "            vertex_label_probs = next_vertex_label_probs\n",
    "            \n",
    "    return vertex_label_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3b8ec",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We can now put together a clustering algorithm from these pieces. This follows exactly the sections: create the graph; cluster the graph; propagate the initial cluster labels through the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5a8b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters(\n",
    "    data,\n",
    "    metric=\"euclidean\",\n",
    "    n_neighbors=30,\n",
    "    min_cluster_size=10,\n",
    "    max_total_weight=64.0, \n",
    "    min_prob=1e-3,\n",
    "    prior=10.0,\n",
    "    noise_prior_prob=0.0, \n",
    "    max_iter=100, \n",
    "    tolerance=1e-5,\n",
    "    k=1,\n",
    "):\n",
    "    prob_graph = construct_prob_graph(\n",
    "        data, \n",
    "        n_neighbors=n_neighbors, \n",
    "        metric=metric, \n",
    "        max_total_weight=max_total_weight, \n",
    "        min_prob=min_prob,\n",
    "        k=k,\n",
    "    )\n",
    "    label_vector = labels_from_prob_graph(prob_graph, min_cluster_size)\n",
    "    cluster_prob_vector = bayesian_label_prop(\n",
    "        prob_graph, label_vector, \n",
    "        noise_prior_prob=noise_prior_prob, \n",
    "        max_iter=max_iter, \n",
    "        tolerance=tolerance,\n",
    "        prior=prior,\n",
    "    )\n",
    "    result = np.argmax(cluster_prob_vector, axis=1)\n",
    "    result[result == label_vector.max() + 1] = -1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d8a2d0f-a2d0-40dd-bc11-6cd3bcf9c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mnist.data\n",
    "metric=\"euclidean\"\n",
    "n_neighbors=30\n",
    "min_cluster_size=800\n",
    "max_total_weight=64.0 \n",
    "min_prob=1e-3\n",
    "prior=10.0\n",
    "noise_prior_prob=0.01 \n",
    "max_iter=100 \n",
    "tolerance=1e-5\n",
    "k=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5efb5338-8019-4e9d-b885-c4412db67ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_graph = construct_prob_graph(\n",
    "    data, \n",
    "    n_neighbors=n_neighbors, \n",
    "    metric=metric, \n",
    "    max_total_weight=max_total_weight, \n",
    "    min_prob=min_prob,\n",
    "    k=k,\n",
    ")\n",
    "label_vector = labels_from_prob_graph(prob_graph, min_cluster_size)\n",
    "cluster_prob_vector = bayesian_label_prop(\n",
    "    prob_graph, label_vector, \n",
    "    noise_prior_prob=noise_prior_prob, \n",
    "    max_iter=max_iter, \n",
    "    tolerance=tolerance,\n",
    "    prior=prior,\n",
    ")\n",
    "result = np.argmax(cluster_prob_vector, axis=1)\n",
    "result[result == label_vector.max() + 1] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4e29e789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9272125561175752"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.adjusted_rand_score(mnist.target[result >= 0], result[result >= 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8bcda073-6ecc-4975-bb95-a6dfd42638ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525428571428571"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(result >= 0) / result.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0f8a76b5-ed76-432d-a5ed-91822557015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_to_keep = np.array([x<=0.9 for x in np.random.random_sample(data.shape[0])])\n",
    "data0 = data[which_to_keep]\n",
    "data_new = data[~which_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d526e529-53f5-490c-b581-33cda52d779d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 29 12:20:36 2023 Building RP forest with 21 trees\n",
      "Tue Aug 29 12:20:37 2023 NN descent for 16 iterations\n",
      "\t 1  /  16\n",
      "\t 2  /  16\n",
      "\t 3  /  16\n",
      "\tStopping threshold met -- exiting after 3 iterations\n",
      "CPU times: user 2min 43s, sys: 24.8 s, total: 3min 8s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nn_index = pynndescent.NNDescent(data0, metric=metric, n_neighbors=2 * n_neighbors, verbose=True)\n",
    "nn_inds, nn_dists = nn_index.neighbor_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2bdde62f-6d57-4ff3-8ec4-156b277e0806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62859, 60)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "707d074a-f778-4ca2-825d-8b42ae39e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = build_models(nn_inds, nn_dists[:, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "094f4d30-7ee6-4b03-bd54-78d58cee5ed6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.09605090e+03, 1.22266335e+02, 6.71438125e+04, 7.73606560e+07,\n",
       "        5.90000000e+01],\n",
       "       [9.33732483e+02, 1.03214088e+02, 5.39985625e+04, 4.99931440e+07,\n",
       "        5.90000000e+01],\n",
       "       [4.17396088e+02, 6.56829453e+01, 2.49469668e+04, 1.07984160e+07,\n",
       "        5.90000000e+01],\n",
       "       ...,\n",
       "       [9.13740051e+02, 1.28390305e+02, 5.59037969e+04, 5.40132400e+07,\n",
       "        5.90000000e+01],\n",
       "       [9.94172180e+02, 1.43832703e+02, 6.25895977e+04, 6.74484000e+07,\n",
       "        5.90000000e+01],\n",
       "       [9.74423828e+02, 1.37074188e+02, 5.96473047e+04, 6.17504640e+07,\n",
       "        5.90000000e+01]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ccf8427b-c0d7-4263-a0f5-d785d1fe908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_edges = np.asarray(\n",
    "        build_edges(nn_inds, nn_dists, models, max_total_weight=max_total_weight, min_prob=min_prob)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9a96a22f-94bc-41dd-8c75-795c96a30afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = scipy.sparse.coo_matrix(\n",
    "    (graph_edges.T[2], (graph_edges.T[0].astype(np.int32), graph_edges.T[1].astype(np.int32))),\n",
    "    shape=(data.shape[0], data.shape[0])\n",
    ")\n",
    "result.eliminate_zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "982c697c-bb95-4498-aa4f-3da461124d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[48688, 41441,  3958, ...,  4278, 57903, 24554],\n",
       "       [17996, 21492, 36138, ..., 57333, 18197, 37415],\n",
       "       [58114, 14695, 10965, ..., 11169, 44819, 56331],\n",
       "       ...,\n",
       "       [54184,  5621,  1325, ...,  6185, 49496, 53634],\n",
       "       [ 1946, 34142, 33163, ..., 16267,  9708, 17132],\n",
       "       [51920, 38317, 25115, ..., 19537, 28004, 61676]], dtype=int32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a35f2910-7da0-4ba9-b938-1e64c1af058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 29 12:22:18 2023 Worst tree score: 0.88798740\n",
      "Tue Aug 29 12:22:18 2023 Mean tree score: 0.89522810\n",
      "Tue Aug 29 12:22:18 2023 Best tree score: 0.90181199\n",
      "Tue Aug 29 12:22:19 2023 Forward diversification reduced edges from 3771540 to 624879\n",
      "Tue Aug 29 12:22:19 2023 Reverse diversification reduced edges from 624879 to 624879\n",
      "Tue Aug 29 12:22:19 2023 Degree pruning reduced edges from 757860 to 757860\n",
      "Tue Aug 29 12:22:19 2023 Resorting data and graph based on tree order\n",
      "Tue Aug 29 12:22:19 2023 Compressing index by removing unneeded attributes\n",
      "Tue Aug 29 12:22:19 2023 Building and compiling search function\n"
     ]
    }
   ],
   "source": [
    "neighbors = nn_index.query(data_new, k = (2 * n_neighbors) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7be12faa-23ad-4983-8453-133f531be740",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inds_tmp = np.array([i+models.shape[0] for i in range(data_new.shape[0])])\n",
    "new_dists_tmp = np.zeros(data_new.shape[0], dtype=np.float32)\n",
    "new_inds = new_inds_tmp[:, np.newaxis]\n",
    "new_dists = new_dists_tmp[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ce2e6c9c-6710-496f-a7d1-64ff98f44c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_inds_new = np.hstack([new_inds, neighbors[0]])\n",
    "nn_dists_new = np.hstack([new_dists, neighbors[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "3fd17275-793c-465a-8379-12e15fc90946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the models (using V(X) = E(X^2) - E(X)^2 so we can stream through the data)\n",
    "# Quirk: We use the 2-out neighborhood and accept the double counting of neighbors of neightbors\n",
    "#        that overlap. The 2-out lets us see further; the double counting biases toward \n",
    "#        the coherent local region as opposed to outliers. Also it just works better. Better\n",
    "#        options or justifications are more than welcome.\n",
    "@numba.njit(fastmath=True)\n",
    "def build_new_models(nn_inds_new, nn_dists, models):\n",
    "    new_result = np.zeros((nn_inds_new.shape[0], 5), dtype=np.float32)\n",
    "    sums = np.zeros(nn_inds_new.shape[0], dtype=np.float32)\n",
    "    sums_of_squares = np.zeros(nn_inds_new.shape[0], dtype=np.float32)\n",
    "    counts = np.zeros(nn_inds_new.shape[0], dtype=np.float32)\n",
    "    # Get sums and counts for the 1-out (not including the points own 1-nn dist)\n",
    "    for i in range(nn_inds_new.shape[0]):\n",
    "        for j in range(1, nn_inds_new.shape[1]):\n",
    "            k = nn_inds_new[i, j]\n",
    "            if k != i and nn_dists[k] > 0.0: # Skip zero dists since they don't fit the model\n",
    "                d = nn_dists[k]\n",
    "                sums[i] += d\n",
    "                sums_of_squares[i] += d * d\n",
    "                counts[i] += 1.0\n",
    " \n",
    "    new_result[:, 2] = sums\n",
    "    new_result[:, 3] = sums_of_squares\n",
    "    new_result[:, 4] = counts\n",
    "    \n",
    "    sums = np.append(models[:,2], sums) \n",
    "    sums_of_squares = np.append(models[:,3], sums_of_squares) \n",
    "    counts = np.append(models[:,4], counts) \n",
    "    \n",
    "    # Total up totals for the 2-out then compute the mean and std\n",
    "    for i in range(nn_inds_new.shape[0]):\n",
    "        count = 0\n",
    "        for j in range(nn_inds_new.shape[1]):\n",
    "            k = nn_inds_new[i, j]\n",
    "            new_result[i, 0] += sums[k]\n",
    "            new_result[i, 1] += sums_of_squares[k]\n",
    "            count += counts[k]\n",
    "            \n",
    "        new_result[i, 0] /= count\n",
    "        new_result[i, 1] = np.sqrt(new_result[i, 1] / count - new_result[i, 0] ** 2)\n",
    "    \n",
    "    # result = np.vstack([models, new_result])\n",
    "        \n",
    "    return new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5221c854-7906-4306-87ad-104073b1993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_new = build_new_models(nn_inds_new, nn_dists[:, k], models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5bedf467-6a36-4e19-ae7a-7750980ac7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_edges_new = np.asarray(\n",
    "        build_edges(nn_inds_new, nn_dists_new, models_new, max_total_weight=max_total_weight, min_prob=min_prob)\n",
    "    ) \n",
    "graph_edges_new[: ,0] +=  models.shape[0]*np.ones(graph_edges_new.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "773b3003-7ee8-47eb-805c-f358ff69241f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.2859e+04, 6.2859e+04, 1.0000e+00],\n",
       "       [6.2859e+04, 6.2859e+04, 1.0000e+00],\n",
       "       [6.2860e+04, 6.2860e+04, 1.0000e+00],\n",
       "       ...,\n",
       "       [6.9998e+04, 6.9998e+04, 1.0000e+00],\n",
       "       [6.9999e+04, 6.9999e+04, 1.0000e+00],\n",
       "       [6.9999e+04, 6.9999e+04, 1.0000e+00]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_edges_new[graph_edges_new[:,0]==graph_edges_new[:,1], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2efa71-54f8-4f1c-80a7-1118900ceb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_prob_graph(new_data, prob_graph, n_neighbors=30, metric=\"euclidean\", max_total_weight=32.0, min_prob=1e-3, k=1):\n",
    "    nn_index = prob_graph.nn_index.update(new_data)\n",
    "    pynndescent.NNDescent(data, metric=metric, n_neighbors=2 * n_neighbors)\n",
    "    nn_inds, nn_dists = nn_index.neighbor_graph\n",
    "    models = build_models(nn_inds, nn_dists[:, k])\n",
    "    \n",
    "    graph_edges = np.asarray(\n",
    "        build_edges(nn_inds, nn_dists, models, max_total_weight=max_total_weight, min_prob=min_prob)\n",
    "    )\n",
    "    result = scipy.sparse.coo_matrix(\n",
    "        (graph_edges.T[2], (graph_edges.T[0].astype(np.int32), graph_edges.T[1].astype(np.int32))),\n",
    "        shape=(data.shape[0], data.shape[0])\n",
    "    )\n",
    "    result.eliminate_zeros()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7cabb4-2a5a-4c5d-bf38-e9b2dca79a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_clusters(\n",
    "    data,\n",
    "    prev_\n",
    "    metric=\"euclidean\",\n",
    "    n_neighbors=30,\n",
    "    min_cluster_size=10,\n",
    "    max_total_weight=64.0, \n",
    "    min_prob=1e-3,\n",
    "    prior=10.0,\n",
    "    noise_prior_prob=0.0, \n",
    "    max_iter=100, \n",
    "    tolerance=1e-5,\n",
    "    k=1,\n",
    "):\n",
    "    prob_graph = construct_prob_graph(\n",
    "        data, \n",
    "        n_neighbors=n_neighbors, \n",
    "        metric=metric, \n",
    "        max_total_weight=max_total_weight, \n",
    "        min_prob=min_prob,\n",
    "        k=k,\n",
    "    )\n",
    "    label_vector = labels_from_prob_graph(prob_graph, min_cluster_size)\n",
    "    cluster_prob_vector = bayesian_label_prop(\n",
    "        prob_graph, label_vector, \n",
    "        noise_prior_prob=noise_prior_prob, \n",
    "        max_iter=max_iter, \n",
    "        tolerance=tolerance,\n",
    "        prior=prior,\n",
    "    )\n",
    "    result = np.argmax(cluster_prob_vector, axis=1)\n",
    "    result[result == label_vector.max() + 1] = -1\n",
    "    \n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HighDimensionalClustering",
   "language": "python",
   "name": "highdimensionalclustering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
