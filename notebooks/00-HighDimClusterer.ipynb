{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed413bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba\n",
    "import sklearn.datasets\n",
    "import pynndescent\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import scipy.sparse\n",
    "import sklearn.metrics\n",
    "from matplotlib import pyplot\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from joblib import Parallel, delayed\n",
    "from time import time, ctime\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "sns.set(rc={\"figure.figsize\":(12, 12)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116c1b5",
   "metadata": {},
   "source": [
    "## Build the Graph\n",
    "\n",
    "The goal is to contruct a graph with edge weights given by estimating the probability of being the nearest neighbor. How can we estimate such a probability? Surely the sample is either the nearest neighbor or it isn't? We assume sampling has been random and somewhat noisy; but that the distribution of samples is *locally* uniform. In other words we assume that in a local region there is a distribution of distances to the nearest neighbor. This distribution is asymptotically a Gamma distribution; since we are in high dimensions this can be well approximated by a normal distribution (which is much cheaper to model, and to compute probabilities for). Thus for each sample we consider it's local neighborhood and fit a model of the distance to the nearest neighbor for samples in that neighborhood. Given such a model we can then compute the probability that the nearest neighbor of the sample is at least as far away as any given sample, and thus create an edge with w weight given by the probability that this point would have been the nearest neighbor under our model. This provides a (directed!) graph with proabiulities assigned to edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca59c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit(fastmath=True)\n",
    "def update_model_using_sample_estimate(mu_0, nu, alpha, beta, sample_mean, sample_var, sample_n):\n",
    "    \n",
    "    alpha = alpha + sample_n/2\n",
    "    beta = beta + (sample_n * sample_var) / 2 + (sample_n * nu * (mu_0 - sample_mean)**2 / (2 * (sample_n + nu)))\n",
    "    mu = (mu_0 * nu + sample_mean * sample_n) / (nu + sample_n)\n",
    "    sigma = np.sqrt(beta / (alpha + 1))\n",
    "    \n",
    "    return mu, sigma\n",
    "\n",
    "@numba.njit()\n",
    "def propagate_models(nn_inds, model_mus, model_sigmas, model_nus, model_alphas, model_betas):\n",
    "    \n",
    "    result_mus = model_mus.copy()\n",
    "    result_sigmas = model_sigmas.copy()\n",
    "    result_nus = model_nus.copy()\n",
    "    result_alphas = model_alphas.copy()\n",
    "    result_betas = model_betas.copy()\n",
    "    \n",
    "    sample_n = nn_inds.shape[1] - 1\n",
    "    \n",
    "    for i in range(nn_inds.shape[0]):\n",
    "        sample_mean = 0.0\n",
    "        sample_var = 0.0\n",
    "        for j in range(nn_inds.shape[1]):\n",
    "            k = nn_inds[i, j]\n",
    "            if k == i:\n",
    "                continue\n",
    "                \n",
    "#             sample_mean += model_mus[k]\n",
    "#             sample_var += model_mus[k]**2\n",
    "            \n",
    "#         sample_mean /= sample_n\n",
    "#         sample_var = sample_var / sample_n - sample_mean ** 2\n",
    "#         result_alphas[i] += sample_n / 2.0\n",
    "#         result_betas[i] += (\n",
    "#             (sample_n * sample_var) / 2 + \n",
    "#             (sample_n * model_nus[i] * (model_mus[i] - sample_mean)**2 / (2 * (sample_n + model_nus[i])))\n",
    "#         )\n",
    "#         result_mus[i] = (model_mus[i] * model_nus[i] + sample_mean * sample_n) / (model_nus[i] + sample_n)\n",
    "#         result_sigmas[i] = result_betas[i] / (result_alphas[i] + 1)\n",
    "#         result_nus[i] += sample_n\n",
    "\n",
    "            \n",
    "#             if model_sigmas[k] > 0:\n",
    "#                 d = nn_dists[i, j]\n",
    "#                 erfc_input = (d - model_mus[k]) / (np.sqrt(2 * model_sigmas[k]))\n",
    "#                 p = math.erfc(erfc_input) / 2.0\n",
    "#             else:\n",
    "#                 p = 1.0\n",
    "                \n",
    "#             nu = p * model_nus[k]\n",
    "#             result_alphas[i] += nu / 2.0\n",
    "#             result_betas[i] += (\n",
    "#                 nu * model_sigmas[k] / 2.0 + \n",
    "#                 nu * result_nus[i] * (result_mus[i] - model_mus[k]) ** 2 /\n",
    "#                 (2 * (nu + result_nus[i]))\n",
    "#             )\n",
    "#             result_mus[i] = (result_mus[i] * result_nus[i] + model_mus[k] * nu) / (result_nus[i] + nu)\n",
    "#             result_sigmas[i] = result_betas[i] / (result_alphas[i] + 1)\n",
    "#             result_nus[i] += nu\n",
    "            \n",
    "            result_alphas[i] += model_nus[k] / 2.0\n",
    "            result_betas[i] += (\n",
    "                model_nus[k] * model_sigmas[k] / 2.0 + \n",
    "                model_nus[k] * result_nus[i] * (result_mus[i] - model_mus[k]) ** 2 /\n",
    "                (2 * (model_nus[k] + result_nus[i]))\n",
    "            )\n",
    "            result_mus[i] = (result_mus[i] * result_nus[i] + model_mus[k] * model_nus[k]) / (result_nus[i] + model_nus[k])\n",
    "            result_sigmas[i] = result_betas[i] / (result_alphas[i] + 1)\n",
    "            result_nus[i] += model_nus[k]\n",
    "        \n",
    "    return result_mus, result_sigmas, result_nus, result_alphas, result_betas\n",
    "\n",
    "@numba.njit()\n",
    "def get_models_params(nn_inds, nn_dists, n_iter=3, prior_strength=0.05):\n",
    "    model_mus = (1.0 - prior_strength) * nn_dists + prior_strength * np.mean(nn_dists)\n",
    "    model_sigmas = np.zeros_like(nn_dists)\n",
    "    model_nus = np.ones_like(nn_dists)# + prior_strength\n",
    "    model_alphas = np.ones_like(nn_dists) / 2.0\n",
    "    model_betas = np.var(nn_dists) * model_alphas # np.zeros_like(nn_dists)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        model_mus, model_sigmas, model_nus, model_alphas, model_betas = propagate_models(\n",
    "            nn_inds, model_mus, model_sigmas, model_nus, model_alphas, model_betas\n",
    "        )\n",
    "    return model_mus, model_sigmas, model_nus, model_alphas, model_betas\n",
    "        \n",
    "@numba.njit()\n",
    "def build_models_prop(nn_inds, nn_dists, n_iter=3, prior_strength=0.05):\n",
    "    \n",
    "    model_mus = (1.0 - prior_strength) * nn_dists + prior_strength * np.mean(nn_dists)\n",
    "    model_sigmas = np.zeros_like(nn_dists)\n",
    "    model_nus = np.ones_like(nn_dists)# + prior_strength\n",
    "    model_alphas = np.ones_like(nn_dists) / 2.0\n",
    "    model_betas = np.var(nn_dists) * model_alphas # np.zeros_like(nn_dists)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        model_mus, model_sigmas, model_nus, model_alphas, model_betas = propagate_models(\n",
    "            nn_inds, model_mus, model_sigmas, model_nus, model_alphas, model_betas\n",
    "        )\n",
    "    \n",
    "    return np.vstack((model_mus, np.sqrt(model_sigmas))).T\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def build_models_old(nn_inds, nn_dists, prior_strength=0.05):\n",
    "    prior_mean = np.mean(nn_dists)\n",
    "    prior_var = np.var(nn_dists)\n",
    "    \n",
    "    prior_nu = prior_strength * nn_inds.shape[1] ** 2\n",
    "    prior_alpha = prior_nu / 2.0\n",
    "    prior_beta = prior_var * prior_alpha\n",
    "    \n",
    "    result = np.zeros((nn_inds.shape[0], 2), dtype=np.float32)\n",
    "    sums = np.zeros(nn_inds.shape[0], dtype=np.float32)\n",
    "    sums_of_squares = np.zeros(nn_inds.shape[0], dtype=np.float32)\n",
    "    counts = np.zeros(nn_inds.shape[0], dtype=np.float32)\n",
    "    # Get sums and counts for the 1-out (not including the points own 1-nn dist)\n",
    "    for i in range(nn_inds.shape[0]):\n",
    "        for j in range(1, nn_inds.shape[1]):\n",
    "            k = nn_inds[i, j]\n",
    "            if k != i and nn_dists[k] > 0.0: # Skip zero dists since they don't fit the model\n",
    "                d = nn_dists[k]\n",
    "                sums[i] += d\n",
    "                sums_of_squares[i] += d * d\n",
    "                counts[i] += 1.0\n",
    "                \n",
    "    # Total up totals for the 2-out then compute the mean and std\n",
    "    for i in range(nn_inds.shape[0]):\n",
    "        count = 0\n",
    "        for j in range(nn_inds.shape[1]):\n",
    "            k = nn_inds[i, j]\n",
    "            result[i, 0] += sums[k]\n",
    "            result[i, 1] += sums_of_squares[k]\n",
    "            count += counts[k]\n",
    "            \n",
    "        result[i, 0] /= count\n",
    "        result[i, 1] = result[i, 1] / count - result[i, 0] ** 2\n",
    "        \n",
    "        # Update to prior\n",
    "        result[i] = update_model_using_sample_estimate(prior_mean, prior_nu, prior_alpha, prior_beta, result[i, 0], result[i, 1], count)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Create an edge list of the (directed!) prob of being a nearest neighbor\n",
    "# This amounts to just computing the relevant prob of the relevant normal using erfc from\n",
    "# the math library.\n",
    "@numba.njit(fastmath=True, parallel=True)\n",
    "def build_edges(nn_inds, nn_dists, models, max_total_weight=32.0, min_prob=1e-3):\n",
    "    result_rows = np.zeros(nn_inds.shape[0] * nn_inds.shape[1], dtype=np.int32)\n",
    "    result_cols = np.zeros(nn_inds.shape[0] * nn_inds.shape[1], dtype=np.int32)\n",
    "    result_vals = np.zeros(nn_inds.shape[0] * nn_inds.shape[1], dtype=np.float32)\n",
    "    root_two = np.sqrt(2)\n",
    "    for i in numba.prange(nn_inds.shape[0]):\n",
    "        result_idx = i * nn_inds.shape[1]\n",
    "        mean = models[i, 0]\n",
    "        std = models[i, 1]\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for j in range(nn_inds.shape[1]):\n",
    "            k = nn_inds[i, j]\n",
    "            if nn_dists[i, j] == 0:\n",
    "                if i != k:\n",
    "                    result_rows[result_idx] = i\n",
    "                    result_cols[result_idx] = k\n",
    "                    result_vals[result_idx] = 1.0\n",
    "                    total_weight += 1.0\n",
    "                    result_idx += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            d = nn_dists[i, j]\n",
    "            if std > 0:\n",
    "                erfc_input = (d - mean) / (root_two * std)\n",
    "                val = math.erfc(erfc_input) / 2.0\n",
    "            else:\n",
    "                val = 0.0\n",
    "\n",
    "            total_weight += val\n",
    "            if total_weight > max_total_weight or val < min_prob:\n",
    "                break\n",
    "            else:\n",
    "                result_rows[result_idx] = i\n",
    "                result_cols[result_idx] = k\n",
    "                result_vals[result_idx] = val\n",
    "                result_idx += 1\n",
    "\n",
    "    return result_rows, result_cols, result_vals\n",
    "\n",
    "@numba.njit(fastmath=True, parallel=True)\n",
    "def build_edges_csr(nn_inds, nn_dists, models, max_total_weight=32.0, min_prob=1e-3):\n",
    "    indptr = np.arange(0, nn_inds.shape[0] * nn_inds.shape[1] + 1, nn_inds.shape[1])\n",
    "    indices = np.zeros(nn_inds.shape[0] * nn_inds.shape[1], dtype=np.int32)\n",
    "    data = np.zeros(nn_inds.shape[0] * nn_inds.shape[1], dtype=np.float32)\n",
    "    \n",
    "    root_two = np.sqrt(2)\n",
    "    for i in numba.prange(nn_inds.shape[0]):\n",
    "        result_idx = i * nn_inds.shape[1]\n",
    "        mean = models[i, 0]\n",
    "        std = models[i, 1]\n",
    "        total_weight = 0.0\n",
    "\n",
    "        for j in range(nn_inds.shape[1]):\n",
    "            k = nn_inds[i, j]\n",
    "            if nn_dists[i, j] == 0:\n",
    "                if i != k:\n",
    "                    indices[result_idx] = k\n",
    "                    data[result_idx] = 1.0\n",
    "                    total_weight += 1.0\n",
    "                    result_idx += 1\n",
    "                continue\n",
    "\n",
    "            d = nn_dists[i, j]\n",
    "            if std > 0:\n",
    "                erfc_input = (d - mean) / (root_two * std)\n",
    "                val = math.erfc(erfc_input) / 2.0\n",
    "            else:\n",
    "                val = 0.0\n",
    "\n",
    "            total_weight += val\n",
    "            if total_weight > max_total_weight or val < min_prob:\n",
    "                break\n",
    "            else:\n",
    "                indices[result_idx] = k\n",
    "                data[result_idx] = val\n",
    "                result_idx += 1\n",
    "\n",
    "    return indptr, indices, data\n",
    "\n",
    "def construct_prob_graph(\n",
    "    data,\n",
    "    n_neighbors=30,\n",
    "    metric=\"euclidean\",\n",
    "    max_total_weight=64.0,\n",
    "    min_prob=1e-3,\n",
    "    k=1,\n",
    "    m=30,\n",
    "    prior_strength=0.05,\n",
    "    n_iter=2,\n",
    "    return_index=False,\n",
    "):\n",
    "    nn_index = pynndescent.NNDescent(data, metric=metric, n_neighbors=n_neighbors, n_trees=8, max_candidates=20)\n",
    "    nn_inds, nn_dists = nn_index.neighbor_graph\n",
    "    models = build_models_prop(\n",
    "        nn_inds[:, :m], nn_dists[:, k], n_iter=n_iter, prior_strength=prior_strength\n",
    "    )\n",
    "\n",
    "#     graph_edges = build_edges(\n",
    "#         nn_inds,\n",
    "#         nn_dists,\n",
    "#         models,\n",
    "#         max_total_weight=max_total_weight,\n",
    "#         min_prob=min_prob,\n",
    "#     )\n",
    "#     result = scipy.sparse.coo_matrix(\n",
    "#         (\n",
    "#             graph_edges[2],\n",
    "#             (graph_edges[0], graph_edges[1]),\n",
    "#         ),\n",
    "#         shape=(data.shape[0], data.shape[0]),\n",
    "#     )\n",
    "    graph_edges = build_edges_csr(\n",
    "        nn_inds,\n",
    "        nn_dists,\n",
    "        models,\n",
    "        max_total_weight=max_total_weight,\n",
    "        min_prob=min_prob,\n",
    "    )\n",
    "    result = scipy.sparse.csr_matrix(\n",
    "        (\n",
    "            graph_edges[2], graph_edges[1], graph_edges[0]\n",
    "        ),\n",
    "        shape=(data.shape[0], data.shape[0]),\n",
    "    )\n",
    "    result.eliminate_zeros()\n",
    "\n",
    "    if return_index:\n",
    "        return (result, nn_index)\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9616bbf",
   "metadata": {},
   "source": [
    "## Cluster the graph\n",
    "\n",
    "We can run single linkage clustering on the resulting graph, and then use HDBSCAN style condensed tree approaches to simplify and get some clusters out. Unfortunately we have a directed graph that may not have a single connected component; that means we need to be able to construct minimal spanning forests, convert them to a set of merge trees, and then run HDBSCAN style tree condensing on the forests of merge trees. None of this is hard, but it is finicky, and we want it all to run fast. Before we get started then we need to build some tools -- specifically a disjoint-set/union-find system which will be used repeatedly in what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6585c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "RankDisjointSet = namedtuple(\"DisjointSet\", [\"parent\", \"rank\"])\n",
    "SizeDisjointSet = namedtuple(\"DisjointSet\", [\"parent\", \"size\"])\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_rank_create(n_elements):\n",
    "    return RankDisjointSet(np.arange(n_elements, dtype=np.int32), np.zeros(n_elements, dtype=np.int32))\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_size_create(n_elements):\n",
    "    return SizeDisjointSet(np.arange(n_elements, dtype=np.int32), np.ones(n_elements, dtype=np.int32))\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_find(disjoint_set, x):\n",
    "    while disjoint_set.parent[x] != x:\n",
    "        x, disjoint_set.parent[x] = disjoint_set.parent[x], disjoint_set.parent[disjoint_set.parent[x]]\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_union_by_rank(disjoint_set, x, y):\n",
    "    x = ds_find(disjoint_set, x)\n",
    "    y = ds_find(disjoint_set, y)\n",
    "\n",
    "    if x == y:\n",
    "        return\n",
    "\n",
    "    if disjoint_set.rank[x] < disjoint_set.rank[y]:\n",
    "        x, y = y, x\n",
    "\n",
    "    disjoint_set.parent[y] = x\n",
    "    if disjoint_set.rank[x] == disjoint_set.rank[y]:\n",
    "        disjoint_set.rank[x] += 1\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def ds_union_by_size(disjoint_set, x, y):\n",
    "    x = ds_find(disjoint_set, x)\n",
    "    y = ds_find(disjoint_set, y)\n",
    "\n",
    "    if x == y:\n",
    "        return\n",
    "\n",
    "    if disjoint_set.size[x] < disjoint_set.size[y]:\n",
    "        x, y = y, x\n",
    "\n",
    "    disjoint_set.parent[y] = x\n",
    "    disjoint_set.size[x] += disjoint_set.size[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd833b6",
   "metadata": {},
   "source": [
    "Next we need single linkage clustering of the graph. That is most easily done by computing a minimum spanning forest, and then processing that into a forest of merge trees. We could use scipy here, but we want flexibility (for forests instead of trees), and they just use Kruskal's algorithm anyway, which is easy to reproduce here. After that we just need the standard process for converting a spanning tree (or forest) to a merge tree (or forest). In practice I think this can all be simplified to one function, since we are doing the merge work in Kruskal's but for now let's keep things as separate standard algorithms.\n",
    "\n",
    "Note that to use minimum spanning forests, and merge order structures we need things going in the other order than probabilities, so we just use negative logs (after all, it's only order that matters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6afed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def kruskal_minimum_spanning_forest(rows, cols, data, n_vertices):\n",
    "    \n",
    "    result_row = np.zeros(n_vertices - 1, dtype=np.int64)\n",
    "    result_col = np.zeros(n_vertices - 1, dtype=np.int64)\n",
    "    result_data = np.zeros(n_vertices - 1, dtype=np.float32)\n",
    "    mst_edge_idx = 0\n",
    "        \n",
    "    edge_order = np.argsort(data)\n",
    "    disjoint_set = ds_rank_create(n_vertices)\n",
    "    \n",
    "    for n in range(data.shape[0]):\n",
    "        edge_idx = edge_order[n]\n",
    "        i = rows[edge_idx]\n",
    "        j = cols[edge_idx]\n",
    "        \n",
    "        i_component = ds_find(disjoint_set, i)\n",
    "        j_component = ds_find(disjoint_set, j)\n",
    "        \n",
    "        if i_component != j_component:\n",
    "            result_row[mst_edge_idx] = i\n",
    "            result_col[mst_edge_idx] = j\n",
    "            result_data[mst_edge_idx] = data[edge_idx]\n",
    "            mst_edge_idx += 1\n",
    "\n",
    "            if disjoint_set.rank[i_component] < disjoint_set.rank[j_component]:\n",
    "                i_component, j_component = j_component, i_component\n",
    "\n",
    "            disjoint_set.parent[j_component] = i_component\n",
    "            if disjoint_set.rank[i_component] == disjoint_set.rank[j_component]:\n",
    "                disjoint_set.rank[i_component] += 1\n",
    "            \n",
    "            if mst_edge_idx >= n_vertices - 1:\n",
    "                break\n",
    "            \n",
    "    return result_row[:mst_edge_idx], result_col[:mst_edge_idx], result_data[:mst_edge_idx]\n",
    "\n",
    "\n",
    "@numba.njit(parallel=True)\n",
    "def boruvka_internal_loop(indptr, indices, data, component_map, best_prob, best_edge):\n",
    "    \n",
    "    n_vertices = indptr.shape[0] - 1\n",
    "    \n",
    "    for i in numba.prange(n_vertices):\n",
    "        \n",
    "        component = component_map[i]\n",
    "        \n",
    "        for j in range(indptr[i], indptr[i+1]):\n",
    "            k = indices[j]\n",
    "            other_component = component_map[k]\n",
    "            \n",
    "            if component != other_component:\n",
    "                p = data[j]\n",
    "                if p > best_prob[component]:\n",
    "                    best_prob[component] = p\n",
    "                    best_edge[component, 0] = i\n",
    "                    best_edge[component, 1] = k\n",
    "                if p > best_prob[other_component]:\n",
    "                    best_prob[other_component] = p\n",
    "                    best_edge[other_component, 0] = k\n",
    "                    best_edge[other_component, 1] = i\n",
    "                    \n",
    "    return\n",
    "    \n",
    "\n",
    "@numba.njit()\n",
    "def boruvka_minimum_spanning_forest(indptr, indices, data):\n",
    "    \n",
    "    n_vertices = indptr.shape[0] - 1\n",
    "    \n",
    "    best_prob = np.zeros(n_vertices, dtype=np.float32)\n",
    "    best_edge = np.zeros((n_vertices, 2), dtype=np.int32)\n",
    "    component_map = np.arange(n_vertices, dtype=np.int32)\n",
    "    \n",
    "    disjoint_set = ds_rank_create(n_vertices)\n",
    "    \n",
    "    result_row = np.zeros(n_vertices - 1, dtype=np.int64)\n",
    "    result_col = np.zeros(n_vertices - 1, dtype=np.int64)\n",
    "    result_data = np.zeros(n_vertices - 1, dtype=np.float32)\n",
    "    mst_edge_idx = 0\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while mst_edge_idx < n_vertices - 1:\n",
    "        # print(f\"{iteration=}, {np.unique(component_map).shape[0]=}, {mst_edge_idx=}\")\n",
    "        boruvka_internal_loop(indptr, indices, data, component_map, best_prob, best_edge)\n",
    "\n",
    "        found_an_edge = False\n",
    "        for i in range(n_vertices):\n",
    "            if best_prob[i] > 0.0:\n",
    "                found_an_edge = True\n",
    "                from_component = ds_find(disjoint_set, best_edge[i, 0])\n",
    "                to_component = ds_find(disjoint_set, best_edge[i, 1])\n",
    "\n",
    "                if from_component != to_component:\n",
    "                    result_row[mst_edge_idx] = best_edge[i, 0]\n",
    "                    result_col[mst_edge_idx] = best_edge[i, 1]\n",
    "                    result_data[mst_edge_idx] = best_prob[i]\n",
    "                    mst_edge_idx += 1\n",
    "\n",
    "                    if disjoint_set.rank[from_component] < disjoint_set.rank[to_component]:\n",
    "                        from_component, to_component = to_component, from_component\n",
    "\n",
    "                    disjoint_set.parent[to_component] = from_component\n",
    "                    if disjoint_set.rank[from_component] == disjoint_set.rank[to_component]:\n",
    "                        disjoint_set.rank[from_component] += 1\n",
    "                        \n",
    "        for i in range(component_map.shape[0]):\n",
    "            component_map[i] = ds_find(disjoint_set, i)\n",
    "            \n",
    "        best_prob[:] = 0.0\n",
    "        best_edge[:] = 0\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        if not found_an_edge:\n",
    "            break\n",
    "            \n",
    "    return result_row[:mst_edge_idx], result_col[:mst_edge_idx], result_data[:mst_edge_idx]    \n",
    "    \n",
    "\n",
    "LinkageMergeData = namedtuple(\"LinkageMergeData\", [\"parent\", \"size\", \"next\"])\n",
    "\n",
    "@numba.njit()\n",
    "def create_linkage_merge_data(base_size):\n",
    "    parent = np.full(2 * base_size - 1, -1, dtype=np.intp)\n",
    "    size = np.concatenate((np.ones(base_size, dtype=np.intp), np.zeros(base_size - 1, dtype=np.intp)))\n",
    "    next_parent = np.array([base_size], dtype=np.intp)\n",
    "\n",
    "    return LinkageMergeData(parent, size, next_parent)\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def linkage_merge_find(linkage_merge, node):\n",
    "    relabel = node\n",
    "    while linkage_merge.parent[node] != -1 and linkage_merge.parent[node] != node:\n",
    "        node = linkage_merge.parent[node]\n",
    "\n",
    "    linkage_merge.parent[node] = node\n",
    "\n",
    "    # label up to the root\n",
    "    while linkage_merge.parent[relabel] != node:\n",
    "        next_relabel = linkage_merge.parent[relabel]\n",
    "        linkage_merge.parent[relabel] = node\n",
    "        relabel = next_relabel\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def linkage_merge_join(linkage_merge, left, right):\n",
    "    linkage_merge.size[linkage_merge.next[0]] = linkage_merge.size[left] + linkage_merge.size[right]\n",
    "    linkage_merge.parent[left] = linkage_merge.next[0]\n",
    "    linkage_merge.parent[right] = linkage_merge.next[0]\n",
    "    linkage_merge.next[0] += 1\n",
    "\n",
    "\n",
    "@numba.njit()\n",
    "def msf_to_linkage_forest(sorted_mst, n_samples=-1):\n",
    "    result = np.empty((sorted_mst.shape[0], sorted_mst.shape[1] + 1))\n",
    "\n",
    "    if n_samples < 0:\n",
    "        n_samples = sorted_mst.shape[0] + 1\n",
    "        \n",
    "    linkage_merge = create_linkage_merge_data(n_samples)\n",
    "\n",
    "    for index in range(sorted_mst.shape[0]):\n",
    "\n",
    "        left = np.intp(sorted_mst[index, 0])\n",
    "        right = np.intp(sorted_mst[index, 1])\n",
    "        delta = sorted_mst[index, 2]\n",
    "\n",
    "        left_component = linkage_merge_find(linkage_merge, left)\n",
    "        right_component = linkage_merge_find(linkage_merge, right)\n",
    "\n",
    "        if left_component > right_component:\n",
    "            result[index][0] = left_component\n",
    "            result[index][1] = right_component\n",
    "        else:\n",
    "            result[index][1] = left_component\n",
    "            result[index][0] = right_component\n",
    "\n",
    "        result[index][2] = delta\n",
    "        result[index][3] = linkage_merge.size[left_component] + linkage_merge.size[right_component]\n",
    "\n",
    "        linkage_merge_join(linkage_merge, left_component, right_component)\n",
    "\n",
    "    return result\n",
    "\n",
    "def merge_forest_from_graph(prob_graph):\n",
    "    \n",
    "    if scipy.sparse.isspmatrix_coo(prob_graph):\n",
    "        graph_for_linkage = prob_graph\n",
    "    else:\n",
    "        graph_for_linkage = prob_graph.tocoo()\n",
    "        \n",
    "    msf = kruskal_minimum_spanning_forest(\n",
    "        graph_for_linkage.row, graph_for_linkage.col, -np.log(graph_for_linkage.data), graph_for_linkage.shape[0]\n",
    "    )\n",
    "    msf = np.vstack(msf).T\n",
    "    merge_forest = msf_to_linkage_forest(msf, graph_for_linkage.shape[0])\n",
    "    \n",
    "    return merge_forest\n",
    "\n",
    "def merge_forest_from_graph_boruvka(prob_graph):\n",
    "        \n",
    "    if scipy.sparse.isspmatrix_csr(prob_graph):\n",
    "        csr_graph = prob_graph\n",
    "    else:    \n",
    "        csr_graph = prob_graph.tocsr()\n",
    "        \n",
    "    msf = boruvka_minimum_spanning_forest(\n",
    "        csr_graph.indptr, csr_graph.indices, csr_graph.data\n",
    "    )\n",
    "    sort_order = np.argsort(msf[2])[::-1]\n",
    "    msf = np.vstack((msf[0][sort_order], msf[1][sort_order], -np.log(msf[2][sort_order]))).T\n",
    "    merge_forest = msf_to_linkage_forest(msf, prob_graph.shape[0])\n",
    "    \n",
    "    return merge_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b74bb",
   "metadata": {},
   "source": [
    "Next we need to be able to condense trees, and also forests. We can just use the standard ``condense_tree`` code (see fast_hdbscan) with some minor modifications where the number of points isn't automatically/implicitly encoded in the merge tree structure. We then supplement that with the ability to extract out roots of the merge forest, and then simply run ``condense_tree`` for each tree in our merge forest, and glue the whole thing together when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e0f477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit()\n",
    "def bfs_from_hierarchy(hierarchy, bfs_root, num_points):\n",
    "    to_process = [bfs_root]\n",
    "    result = []\n",
    "\n",
    "    while to_process:\n",
    "        result.extend(to_process)\n",
    "        next_to_process = []\n",
    "        for n in to_process:\n",
    "            if n >= num_points:\n",
    "                i = n - num_points\n",
    "                next_to_process.append(int(hierarchy[i, 0]))\n",
    "                next_to_process.append(int(hierarchy[i, 1]))\n",
    "        to_process = next_to_process\n",
    "\n",
    "    return result\n",
    "\n",
    "@numba.njit()\n",
    "def eliminate_branch(branch_node, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore, hierarchy,\n",
    "                     num_points):\n",
    "    if branch_node < num_points:\n",
    "        parents[idx] = parent_node\n",
    "        children[idx] = branch_node\n",
    "        lambdas[idx] = lambda_value\n",
    "        idx += 1\n",
    "    else:\n",
    "        for sub_node in bfs_from_hierarchy(hierarchy, branch_node, num_points):\n",
    "            if sub_node < num_points:\n",
    "                children[idx] = sub_node\n",
    "                parents[idx] = parent_node\n",
    "                lambdas[idx] = lambda_value\n",
    "                idx += 1\n",
    "            else:\n",
    "                ignore[sub_node] = True\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "CondensedTree = namedtuple('CondensedTree', ['parent', 'child', 'lambda_val', 'child_size'])\n",
    "\n",
    "@numba.njit()\n",
    "def neg_exp(x):\n",
    "    return np.exp(-x)\n",
    "\n",
    "@numba.njit(fastmath=True)\n",
    "def condense_tree(hierarchy, min_cluster_size=10, root=-1, num_points=-1, next_label=-1, lambda_func=neg_exp):\n",
    "    \n",
    "    if root < 0:\n",
    "        root = 2 * hierarchy.shape[0]\n",
    "        \n",
    "    if num_points < 0:\n",
    "        num_points = hierarchy.shape[0] + 1\n",
    "        \n",
    "    if next_label < 0:\n",
    "        next_label = num_points\n",
    "\n",
    "    node_list = bfs_from_hierarchy(hierarchy, root, num_points)\n",
    "\n",
    "    relabel = np.zeros(root + 1, dtype=np.int64)\n",
    "    relabel[root] = next_label\n",
    "    \n",
    "    next_label += 1\n",
    "\n",
    "    parents = np.ones(root, dtype=np.int64)\n",
    "    children = np.empty(root, dtype=np.int64)\n",
    "    lambdas = np.empty(root, dtype=np.float32)\n",
    "    sizes = np.ones(root, dtype=np.int64)\n",
    "\n",
    "    ignore = np.zeros(root + 1, dtype=np.bool8)\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for node in node_list:\n",
    "        if ignore[node] or node < num_points:\n",
    "            continue\n",
    "\n",
    "        parent_node = relabel[node]\n",
    "        l, r, d, _ = hierarchy[node - num_points]\n",
    "        left = np.int64(l)\n",
    "        right = np.int64(r)\n",
    "        lambda_value = lambda_func(d)\n",
    "\n",
    "        left_count = np.int64(hierarchy[left - num_points, 3]) if left >= num_points else 1\n",
    "        right_count = np.int64(hierarchy[right - num_points, 3]) if right >= num_points else 1\n",
    "\n",
    "        # The logic here is in a strange order, but it has non-trivial performance gains ...\n",
    "        # The most common case by far is a singleton on the left; and cluster on the right take care of this separately\n",
    "        if left < num_points and right_count >= min_cluster_size:\n",
    "            relabel[right] = parent_node\n",
    "            parents[idx] = parent_node\n",
    "            children[idx] = left\n",
    "            lambdas[idx] = lambda_value\n",
    "            idx += 1\n",
    "        # Next most common is a small left cluster and a large right cluster: \n",
    "        #   relabel the right node; eliminate the left branch\n",
    "        elif left_count < min_cluster_size and right_count >= min_cluster_size:\n",
    "            relabel[right] = parent_node\n",
    "            idx = eliminate_branch(left, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore,\n",
    "                                   hierarchy, num_points)\n",
    "        # Then we have a large left cluster and a small right cluster: relabel the left node; elimiate the right branch\n",
    "        elif left_count >= min_cluster_size and right_count < min_cluster_size:\n",
    "            relabel[left] = parent_node\n",
    "            idx = eliminate_branch(right, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore,\n",
    "                                   hierarchy, num_points)\n",
    "        # If both clusters are small then eliminate all branches\n",
    "        elif left_count < min_cluster_size and right_count < min_cluster_size:\n",
    "            idx = eliminate_branch(left, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore,\n",
    "                                   hierarchy, num_points)\n",
    "            idx = eliminate_branch(right, parent_node, lambda_value, parents, children, lambdas, sizes, idx, ignore,\n",
    "                                   hierarchy, num_points)\n",
    "        # and finally if we actually have a legitimate cluster split, handle that correctly\n",
    "        else:\n",
    "            relabel[left] = next_label\n",
    "\n",
    "            parents[idx] = parent_node\n",
    "            children[idx] = next_label\n",
    "            lambdas[idx] = lambda_value\n",
    "            sizes[idx] = left_count\n",
    "            next_label += 1\n",
    "            idx += 1\n",
    "\n",
    "            relabel[right] = next_label\n",
    "\n",
    "            parents[idx] = parent_node\n",
    "            children[idx] = next_label\n",
    "            lambdas[idx] = lambda_value\n",
    "            sizes[idx] = right_count\n",
    "            next_label += 1\n",
    "            idx += 1\n",
    "\n",
    "    return CondensedTree(parents[:idx], children[:idx], lambdas[:idx], sizes[:idx])\n",
    "\n",
    "@numba.njit()\n",
    "def merge_tree_roots(merge_tree, min_cluster_size, n_samples=-1):\n",
    "    if n_samples < 0:\n",
    "        n_samples = merge_tree.shape[0] + 1\n",
    "        \n",
    "    is_root = np.ones(merge_tree.shape[0], dtype=np.bool_)\n",
    "    for i in range(merge_tree.shape[0]):\n",
    "        if merge_tree[i, 0] >= n_samples:\n",
    "            is_root[int(merge_tree[i, 0] - n_samples)] = False\n",
    "        if merge_tree[i, 1] >= n_samples:\n",
    "            is_root[int(merge_tree[i, 1] - n_samples)] = False\n",
    "        \n",
    "    roots = np.nonzero(is_root)[0] + n_samples\n",
    "    \n",
    "    result = []\n",
    "    for i in range(roots.shape[0]):\n",
    "        if merge_tree[roots[i] - n_samples, 3] > min_cluster_size:\n",
    "            result.append(roots[i])\n",
    "        \n",
    "    return result\n",
    "\n",
    "#@numba.njit()\n",
    "def condense_forest(merge_forest, min_cluster_size=10, n_samples=-1, lambda_func=neg_exp):\n",
    "\n",
    "    if n_samples < 0:\n",
    "        n_samples = merge_tree.shape[0] + 1\n",
    "        \n",
    "    roots = merge_tree_roots(merge_forest, min_cluster_size=min_cluster_size, n_samples=n_samples)\n",
    "    \n",
    "    ctrees = []\n",
    "    next_label = n_samples + 1\n",
    "    for root in roots:\n",
    "        ctree = condense_tree(\n",
    "            merge_forest, \n",
    "            min_cluster_size=min_cluster_size, \n",
    "            root=root, \n",
    "            num_points=n_samples,\n",
    "            next_label=next_label,\n",
    "            lambda_func=lambda_func,\n",
    "        )\n",
    "        next_label = ctree.parent.max() + 1\n",
    "        ctrees.append(ctree)\n",
    "        \n",
    "    result = CondensedTree(\n",
    "        np.concatenate([x.parent for x in ctrees]),\n",
    "        np.concatenate([x.child for x in ctrees]),\n",
    "        np.concatenate([x.lambda_val for x in ctrees]),\n",
    "        np.concatenate([x.child_size for x in ctrees]),\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad3c736",
   "metadata": {},
   "source": [
    "Lastly we need to be able to extract clusters. We could use any technique, but I went with the leaf extraction because it seemed to work better. In principle this means we don't need to condense the whole tree, just the leaves which could save a lot of time/trouble, but other extraction methods are possible so I'll stick with flexibility (and borrowed standard algorithms) for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c788e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def extract_leaves(condensed_tree, allow_single_cluster=True):\n",
    "    n_points = condensed_tree.parent.min()\n",
    "    n_nodes = max(condensed_tree.parent.max() + 1, n_points + 1)\n",
    "    leaf_indicator = np.ones(n_nodes, dtype=np.bool_)\n",
    "    leaf_indicator[:n_points] = False\n",
    "\n",
    "    for parent, child_size in zip(condensed_tree.parent, condensed_tree.child_size):\n",
    "        if child_size > 1:\n",
    "            leaf_indicator[parent] = False\n",
    "\n",
    "    return np.nonzero(leaf_indicator)[0]\n",
    "\n",
    "@numba.njit()\n",
    "def get_cluster_label_vector(\n",
    "        tree,\n",
    "        clusters,\n",
    "):\n",
    "    root_cluster = tree.parent.min()\n",
    "    result = np.empty(root_cluster, dtype=np.intp)\n",
    "    cluster_label_map = {c: n for n, c in enumerate(np.sort(clusters))}\n",
    "\n",
    "    disjoint_set = ds_rank_create(tree.parent.max() + 1)\n",
    "    clusters = set(clusters)\n",
    "\n",
    "    for n in range(tree.parent.shape[0]):\n",
    "        child = tree.child[n]\n",
    "        parent = tree.parent[n]\n",
    "        if child not in clusters:\n",
    "            ds_union_by_rank(disjoint_set, parent, child)\n",
    "\n",
    "    for n in range(root_cluster):\n",
    "        cluster = ds_find(disjoint_set, n)\n",
    "        if cluster in cluster_label_map:\n",
    "            result[n] = cluster_label_map[cluster]\n",
    "        else:\n",
    "            result[n] = -1\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def labels_from_prob_graph(prob_graph, min_cluster_size):\n",
    "    \n",
    "    merge_forest = merge_forest_from_graph_boruvka(prob_graph)\n",
    "#     merge_forest = merge_forest_from_graph(prob_graph)\n",
    "    condensed_forest = condense_forest(merge_forest, min_cluster_size=min_cluster_size, n_samples=prob_graph.shape[0])\n",
    "    chosen_clusters = extract_leaves(condensed_forest)\n",
    "    \n",
    "    result = get_cluster_label_vector(condensed_forest, chosen_clusters)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9615f47",
   "metadata": {},
   "source": [
    "## Propagate labels through the graph\n",
    "\n",
    "The clustering of the graph works well in that it picks out most of the clusters, but it leaves a great deal of data as noise. We can fix that by running a label propagation through the graph. We do still want to keep the ability to label points as noise, and it would be good to keep the propagation soft, so we can do a Bayesian label propagation of probability vectors over possible labels, including a noise label, with a Bayesian update of the distribution at each propagation round. We can then iterate until relative convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b419bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic Bayesian label prop with a few quirks\n",
    "# Quirks: Different vertices have different (weighted) degrees; we normalize so the total outgoing weight is consistent\n",
    "#         We reassert labels each iteration boosting the prior; this includes a level of noise prior which is a parameter\n",
    "#         This is not really theoretically sound, but it prevents labels over-running where they shouldn't. We could in\n",
    "#         theory simply hold labelled vertics fixed; I haven't experimented with that much yet.\n",
    "@numba.njit(fastmath=True, parallel=True)\n",
    "def bayesian_label_prop_iteration(\n",
    "    indptr, indices, probs, vertex_label_probs, labels, degree_correction, row_weights, label_prior=0.9, noise_prior=0.2\n",
    "):\n",
    "    \n",
    "    n_rows = indptr.shape[0] - 1\n",
    "    n_labels = vertex_label_probs.shape[1]\n",
    "    vertex_votes = np.zeros_like(vertex_label_probs, dtype=np.float64)\n",
    "    norms = np.zeros(n_rows)\n",
    "    \n",
    "    if label_prior < 1.0:\n",
    "        label_multiplier = label_prior / (1.0 - label_prior)\n",
    "    else:\n",
    "        label_multiplier = 1e16\n",
    "    if noise_prior < 1.0:\n",
    "        noise_multiplier = noise_prior / (1.0 - noise_prior)\n",
    "    else:\n",
    "        noise_multiplier = 1e16\n",
    "    \n",
    "    for i in numba.prange(n_rows):\n",
    "        if labels[i] >= 0:\n",
    "            vertex_votes[i] = label_multiplier * row_weights[i] * vertex_label_probs[i]\n",
    "            norms[i] += label_multiplier * row_weights[i]\n",
    "        else:\n",
    "            vertex_votes[i] = noise_multiplier * row_weights[i] * vertex_label_probs[i]\n",
    "            norms[i] += noise_multiplier * row_weights[i]\n",
    "        \n",
    "    for i in numba.prange(n_rows):\n",
    "        for k in range(indptr[i], indptr[i+1]):\n",
    "            j = indices[k]\n",
    "            prob = probs[k] * degree_correction[i]\n",
    "            \n",
    "            for l in range(n_labels):\n",
    "                vertex_votes[i, l] += prob * vertex_label_probs[j, l]\n",
    "                \n",
    "            norms[i] += prob\n",
    "        \n",
    "    for i in numba.prange(n_rows):\n",
    "        if norms[i] > 0.0:\n",
    "            vertex_votes[i] /= norms[i]\n",
    "        else:\n",
    "            vertex_votes[i, -1] = 1.0\n",
    "            \n",
    "    return vertex_votes\n",
    "\n",
    "\n",
    "@numba.njit(fastmath=True, parallel=True)\n",
    "def bayesian_label_prop_loop(\n",
    "    indptr, indices, data, degree_correction, row_weights,\n",
    "    label_vector, max_iter=100, tolerance=1e-5, label_prior=0.9, noise_prior=0.2\n",
    "):\n",
    "    n_vertices = indptr.shape[0] - 1\n",
    "    vertex_label_probs = np.zeros((n_vertices, label_vector.max() + 2), dtype=np.float64)\n",
    "    for i in range(vertex_label_probs.shape[0]):\n",
    "        vertex_label_probs[i, label_vector[i]] = 1.0\n",
    "        \n",
    "    for i in range(max_iter):\n",
    "        next_vertex_label_probs = bayesian_label_prop_iteration(\n",
    "            indptr, \n",
    "            indices, \n",
    "            data, \n",
    "            vertex_label_probs, \n",
    "            label_vector,\n",
    "            degree_correction,\n",
    "            row_weights,\n",
    "            noise_prior=noise_prior,\n",
    "            label_prior=label_prior,\n",
    "        )\n",
    "        \n",
    "        total_change = np.sqrt(np.sum((vertex_label_probs - next_vertex_label_probs)**2))\n",
    "        \n",
    "        if total_change / n_vertices < tolerance:\n",
    "            return next_vertex_label_probs\n",
    "        else:\n",
    "            vertex_label_probs = next_vertex_label_probs    \n",
    "    \n",
    "    return vertex_label_probs\n",
    "\n",
    "@numba.njit(parallel=True, fastmath=True)\n",
    "def degree_correction_and_row_weights(indptr, indices, data):\n",
    "    \n",
    "    n_rows = indptr.shape[0] - 1\n",
    "    degrees = np.full(n_rows, 1e-16, dtype=np.float32)\n",
    "    \n",
    "    for n in range(indices.shape[0]):\n",
    "        j = indices[n]\n",
    "        degrees[j] += data[n]\n",
    "        \n",
    "    degree_correction = 1.0 / degrees\n",
    "    row_weights = np.zeros(n_rows, dtype=np.float32)\n",
    "    \n",
    "    for i in numba.prange(n_rows):\n",
    "        for n in range(indptr[i], indptr[i+1]):\n",
    "            j = indices[n]\n",
    "            row_weights[i] += data[n] * degree_correction[j]\n",
    "            \n",
    "    return degree_correction, row_weights\n",
    "\n",
    "def bayesian_label_prop(prob_graph, label_vector, max_iter=100, tolerance=1e-5, label_prior=0.9, noise_prior=0.2):\n",
    "    if not scipy.sparse.isspmatrix_csr(prob_graph):\n",
    "        prob_graph = prob_graph.tocsr()\n",
    "        \n",
    "    degree_correction, row_weights = degree_correction_and_row_weights(\n",
    "        prob_graph.indptr, \n",
    "        prob_graph.indices, \n",
    "        prob_graph.data,\n",
    "    )\n",
    "    \n",
    "    vertex_label_probs = bayesian_label_prop_loop(\n",
    "        prob_graph.indptr, \n",
    "        prob_graph.indices, \n",
    "        prob_graph.data,\n",
    "        degree_correction,\n",
    "        row_weights,\n",
    "        label_vector,\n",
    "        max_iter=max_iter,\n",
    "        tolerance=tolerance,\n",
    "        label_prior=label_prior,\n",
    "        noise_prior=noise_prior,\n",
    "    )\n",
    "            \n",
    "    return vertex_label_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff3b8ec",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "We can now put together a clustering algorithm from these pieces. This follows exactly the sections: create the graph; cluster the graph; propagate the initial cluster labels through the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33950b89-1210-4915-95a4-9ff90de9f477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_models_on_new(nn_inds_new, nn_dists_new, model_mus, model_sigmas, model_nus, model_alphas, model_betas, prior_strength=0.2):\n",
    "    \n",
    "    model_mus_new = (1.0 - prior_strength) * nn_dists_new + prior_strength * np.mean(model_mus)\n",
    "    model_sigmas_new = np.zeros_like(nn_dists_new)\n",
    "    model_nus_new = np.ones_like(nn_dists_new)# + prior_strength\n",
    "    model_alphas_new = np.ones_like(nn_dists_new) / 2.0\n",
    "    model_betas_new = 2 * np.var(model_nus) * model_alphas_new # Start with a larger variance. This should be changed!!!\n",
    "    \n",
    "    for i in range(nn_inds_new.shape[0]):\n",
    "        sample_mean = 0.0\n",
    "        sample_var = 0.0\n",
    "        for j in range(nn_inds_new.shape[1]):\n",
    "            k = nn_inds_new[i, j]  #Updated this from nn_inds\n",
    "            if k == i:\n",
    "                continue\n",
    "            \n",
    "            model_alphas_new[i] += model_nus[k] / 2.0\n",
    "            model_betas_new[i] += (\n",
    "                model_nus[k] * model_sigmas[k] / 2.0 + \n",
    "                model_nus[k] * model_nus_new[i] * (model_mus_new[i] - model_mus[k]) ** 2 /\n",
    "                (2 * (model_nus[k] + model_nus_new[i]))\n",
    "            )\n",
    "            model_mus_new[i] = (model_mus_new[i] * model_nus_new[i] + model_mus[k] * model_nus[k]) / (model_nus_new[i] + model_nus[k])\n",
    "            model_sigmas_new[i] = model_betas_new[i] / (model_alphas_new[i] + 1)\n",
    "            model_nus_new[i] += model_nus[k]\n",
    "        \n",
    "    return model_mus_new, model_sigmas_new, model_nus_new, model_alphas_new, model_betas_new\n",
    "\n",
    "\n",
    "def average_prob_vectors(cluster_prob_vector, new_models, nn_inds_new, nn_dists_new, min_prob=0.01):\n",
    "    cluster_prob_vector_new = np.zeros_like(cluster_prob_vector[:nn_inds_new.shape[0],:])\n",
    "    norm_factor = np.zeros(nn_inds_new.shape[0])\n",
    "    root_two = np.sqrt(2)\n",
    "    noise_vector = np.zeros_like(cluster_prob_vector_new[0,:])\n",
    "    noise_vector[-1] = 1.0\n",
    "\n",
    "    for i in numba.prange(nn_inds_new.shape[0]):\n",
    "        mean = new_models[i, 0]\n",
    "        std = new_models[i, 1]\n",
    "        list_vectors = []\n",
    "        list_weights = []\n",
    "\n",
    "        for j in range(nn_inds_new.shape[1]):\n",
    "            val=-1\n",
    "            k = nn_inds_new[i, j]  # Who is my kth nearest neighbour from the original set\n",
    "            # If I have distance zero to them\n",
    "            if nn_dists_new[i, j] == 0:\n",
    "                # TODO : infinite weight !\n",
    "                val = 10000.0\n",
    "\n",
    "            else:\n",
    "                d = nn_dists_new[i, j]\n",
    "                if std > 0:\n",
    "                    erfc_input = (d - mean) / (root_two * std)\n",
    "                    val = math.erfc(erfc_input) / 2.0\n",
    "                else:\n",
    "                    val = 0.0\n",
    "     \n",
    "                if val < min_prob:\n",
    "                    val = 0.0\n",
    "            #print(val)\n",
    "            #if(val==-1):\n",
    "#                print(f'FOO: val = {val}, i={i}, j={j}, k={k}, first_condition = {nn_dists_new[i, j]}, nn_dist_new={nn_dists_new}, std={std}')\n",
    "            list_vectors.append(cluster_prob_vector[k, :])\n",
    "            list_weights.append(val)\n",
    "            # norm_factor[i] += val\n",
    "            # cluster_prob_vector_new[i, :] += val*cluster_prob_vector[k, :]\n",
    "            \n",
    "        # TODO: Check for all zero weights\n",
    "        if(np.sum(list_weights)==0):\n",
    "            cluster_prob_vector_new[i, :] = noise_vector\n",
    "        else:\n",
    "            cluster_prob_vector_new[i, :] = np.average(np.array(list_vectors), axis=0, weights=np.array(list_weights))\n",
    "        \n",
    "\n",
    "    # new_cluster_vectors =  cluster_prob_vector_new / norm_factor[:, np.newaxis]\n",
    "\n",
    "    return cluster_prob_vector_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fc3f82-ec57-4c49-a92b-a447dd65e465",
   "metadata": {},
   "source": [
    "# Clusterer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ce0b52-1cca-4592-8b09-2f72c2b5180c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClusterMixin\n",
    "\n",
    "class HighDimClusterer(BaseEstimator, ClusterMixin):\n",
    "    def __init__(\n",
    "        self, \n",
    "        metric=\"euclidean\",\n",
    "        n_neighbors=30,\n",
    "        min_cluster_size=10,\n",
    "        max_total_weight=64.0, \n",
    "        min_prob=1e-3,\n",
    "        label_prior=0.9,\n",
    "        noise_prior=0.2, \n",
    "        max_iter=100, \n",
    "        tolerance=1e-5,\n",
    "        k=1,\n",
    "        model_prior_strength=0.2,\n",
    "        n_iter=2,\n",
    "    ):\n",
    "        self.metric = metric\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.max_total_weight = max_total_weight\n",
    "        self.min_prob = min_prob\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.label_prior = label_prior\n",
    "        self.noise_prior = noise_prior\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.k = k\n",
    "        self.model_prior_strength = model_prior_strength\n",
    "        self.n_iter = n_iter\n",
    "    \n",
    "            \n",
    "    #@numba.njit()\n",
    "    def build_models_prop(self, nn_inds, kth_nn_dists):\n",
    "    \n",
    "        model_mus = (1.0 - self.model_prior_strength) * kth_nn_dists + self.model_prior_strength * np.mean(kth_nn_dists)\n",
    "        model_sigmas = np.zeros_like(kth_nn_dists)\n",
    "        model_nus = np.ones_like(kth_nn_dists)# + prior_strength\n",
    "        model_alphas = np.ones_like(kth_nn_dists) / 2.0\n",
    "        model_betas = np.var(kth_nn_dists) * model_alphas # np.zeros_like(nn_dists)\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            model_mus, model_sigmas, model_nus, model_alphas, model_betas = propagate_models(\n",
    "                nn_inds, model_mus, model_sigmas, model_nus, model_alphas, model_betas\n",
    "            )\n",
    "\n",
    "        self._model_alphas = model_alphas\n",
    "        self._model_mus = model_mus\n",
    "        self._model_sigmas = model_sigmas\n",
    "        self._model_nus = model_nus\n",
    "        self._model_betas = model_betas\n",
    "        self._models = np.vstack((model_mus, np.sqrt(model_sigmas))).T\n",
    "\n",
    "    def construct_prob_graph(\n",
    "        self,\n",
    "        data,\n",
    "    ):\n",
    "        m = self.n_neighbors # USed to be a parameter defaulted to 30\n",
    "        self.nn_index_ = pynndescent.NNDescent(data, metric=self.metric, n_neighbors=self.n_neighbors, n_trees=8, max_candidates=20)\n",
    "        nn_inds, nn_dists = self.nn_index_.neighbor_graph\n",
    "        self.build_models_prop(nn_inds[:, :m], nn_dists[:, self.k])\n",
    "\n",
    "        graph_edges = build_edges_csr(\n",
    "            nn_inds,\n",
    "            nn_dists,\n",
    "            self._models,\n",
    "            max_total_weight=self.max_total_weight,\n",
    "            min_prob=self.min_prob,\n",
    "        )\n",
    "        result = scipy.sparse.csr_matrix(\n",
    "            (\n",
    "                graph_edges[2], graph_edges[1], graph_edges[0]\n",
    "            ),\n",
    "            shape=(data.shape[0], data.shape[0]),\n",
    "        )\n",
    "        result.eliminate_zeros()\n",
    "        self.prob_graph_ = result\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.construct_prob_graph(X)\n",
    "        self.label_vector_ = labels_from_prob_graph(self.prob_graph_, self.min_cluster_size)\n",
    "        self.cluster_prob_vector_ = bayesian_label_prop(\n",
    "            self.prob_graph_, \n",
    "            self.label_vector_, \n",
    "            noise_prior=self.noise_prior,\n",
    "            label_prior=self.label_prior,\n",
    "            max_iter=self.max_iter, \n",
    "            tolerance=self.tolerance,\n",
    "        )\n",
    "        self.labels_ = np.argmax(self.cluster_prob_vector_, axis=1)\n",
    "        self.labels_[self.labels_ == self.label_vector_.max() + 1] = -1\n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.labels_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predict_prob_vector = self.predict_proba(X)\n",
    "        labels = np.argmax(predict_prob_vector, axis=1)\n",
    "        labels[labels == predict_prob_vector.shape[1]-1] = -1\n",
    "        return labels\n",
    "    \n",
    "    def predict_prob_max(self, X):\n",
    "        predict_prob_vector = self.predict_proba(X)\n",
    "        prob = np.max(predict_prob_vector, axis=1)\n",
    "        return prob\n",
    "    \n",
    "    def predict_proba(self, X, prob_threshold=None):\n",
    "        \"\"\"\n",
    "        TODO: Replace this with Valerie's actual predict function.  Yay Valerie!\n",
    "        Compute a soft clustering of X under a precomputed clustering model.  \n",
    "        That is a probability vector of each row in X being a member of each cluster.\n",
    "        \n",
    "        X: {array-like} of shape (n_samples, n_features)                \n",
    "            Training vectors, where `n_samples` is the number of samples\n",
    "            and `n_features` is the number of features.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        An np.array of shape (n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        #TODO: Check that the model has been fit\n",
    "        sklearn.utils.validation.check_is_fitted(self, ['_model_mus', 'nn_index_', 'cluster_prob_vector_'])\n",
    "                \n",
    "        #compute which training neighbours are closest to each of our points\n",
    "        neighbors = self.nn_index_.query(X, k = self.n_neighbors - 1)\n",
    "        self.nn_inds_new = neighbors[0]\n",
    "        self.nn_dists_new = neighbors[1]\n",
    "    \n",
    "        model_mus_new, model_sigmas_new, model_nus_new, model_alphas_new, model_betas_new = propagate_models_on_new(\n",
    "            self.nn_inds_new, self.nn_dists_new[:,0], self._model_mus, self._model_sigmas, self._model_nus, self._model_alphas, self._model_betas\n",
    "        )\n",
    "\n",
    "        self.new_models = np.vstack((model_mus_new, np.sqrt(model_sigmas_new))).T\n",
    "\n",
    "        if prob_threshold is None:\n",
    "            prob_threshold = self.min_prob\n",
    "        new_cluster_vectors = average_prob_vectors(self.cluster_prob_vector_, self.new_models, self.nn_inds_new , self.nn_dists_new, min_prob=prob_threshold)\n",
    "\n",
    "        return new_cluster_vectors\n",
    "        \n",
    "    def naive_predict(self, X):\n",
    "        predict_prob_vector = self.naive_predict_prob_vector(X)\n",
    "        labels = np.argmax(predict_prob_vector, axis=1)\n",
    "        labels[labels == predict_prob_vector.shape[1]-1] = -1\n",
    "        return labels\n",
    "        \n",
    "    def naive_predict_proba(self, X):\n",
    "        predict_prob_vector = self.naive_predict_prob_vector(X)\n",
    "        prob = np.max(predict_prob_vector, axis=1)\n",
    "        return prob\n",
    "\n",
    "    def naive_predict_prob_vector(self, X):\n",
    "        \n",
    "#         # Use the prob_graph_ to compute the likelihood vector for each of these new points\n",
    "#         # Should we add the points and their distances to the graph?  Or just query against it?  \n",
    "        \n",
    "#         # Do some label propegation.\n",
    "#         # Should we use the same number of iterations?  Should that be a predict paramter?  Might want to test this.\n",
    "        \n",
    "#         #In the mean time something very naive and predict the distance weighted average of each points neighbours label prob vector.\n",
    "        n_index, n_dist = self.nn_index_.query(X)\n",
    "        n_weight = normalize(n_dist, axis=1, norm='l1')\n",
    "        predict_prob_vector = np.array([np.average(self.cluster_prob_vector_[n_index[i]], axis=0, weights=n_weight[i]) for i in range(len(n_index))])\n",
    "        return predict_prob_vector    \n",
    "\n",
    "    def score(self, X, y):\n",
    "        from sklearn.metrics import adjusted_rand_score\n",
    "        return adjusted_rand_score(y, self.predict(X))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474cfa3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Experiments\n",
    "\n",
    "We'll run this algorithm (with tuned parameters) and compare it with a combination of UMAP and HDBSCAN (with tuned parameters). This algorithm was developed largely against MNIST, so it is perhaps somewhat overtuned to MNIST. You may want to substitute in your own favourite dataset instead; alhtough it should be \"high dimensional\" enough the not require Gamma distribution models (above about 10 should probably be enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bef9e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/home/vmpouli/.conda/envs/prob_high_d_clustering/lib/python3.11/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist = sklearn.datasets.fetch_openml(\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d12e9996-08d8-4d6e-bb0e-e739c30d6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'min_cluster_size' : 800,\n",
    "    'n_neighbors' : 15,\n",
    "    'max_iter' : 50,\n",
    "    'label_prior' : 0.99,\n",
    "    'noise_prior' : 0.01,\n",
    "    'k' : 1,\n",
    "    'min_prob' : 1e-4,\n",
    "    'model_prior_strength' : 0.0,\n",
    "    'n_iter' : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7708f9a8-19fc-4c21-acdd-82c828e0d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = HighDimClusterer(**parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4bf9f7e1-2121-4a28-adbf-c27dc3cfd83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = clusterer.fit_predict(mnist.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2352a3b1-8278-4f7f-b274-df087a6dff5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9010181368899497"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.adjusted_rand_score(cluster_labels, mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96fe50a-4fff-475d-bb28-3f4813ee09d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob_high_d_clustering",
   "language": "python",
   "name": "prob_high_d_clustering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
